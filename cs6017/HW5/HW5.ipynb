{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitney Kenner\n",
    "u0777962\n",
    "HW5\n",
    "7/2/23\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup. \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn import tree, svm, metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to scale images...\n",
    "\n",
    "digits = load_digits()\n",
    "X = scale( digits.data )\n",
    "y = digits.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "n_digits = len( np.unique(digits.target) )\n",
    "\n",
    "print( \"n_digits: %d, n_samples %d, n_features %d\" % (n_digits, n_samples, n_features) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is what one digit (the \"zero\") looks like:\n",
    "\n",
    "print( \"===\\nThe raw data\" )\n",
    "print( digits.images[0] )\n",
    "\n",
    "print( \"===\\nThe scaled data\" )\n",
    "print( X[0] )\n",
    "\n",
    "print( \"===\\nThe digit\" )\n",
    "print( digits.target[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 25 images...\n",
    "\n",
    "plt.figure( figsize=(10, 10) )\n",
    " \n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( np.reshape( X[ii,:], (8,8) ), cmap='Greys', interpolation='nearest' )\n",
    "    plt.axis( 'off' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Classification with Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( X, y, random_state=1, test_size=0.8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SVM with an rbf kernel and the cost parameter C=5 to build a classifier using the training dataset.\n",
    "model = svm.SVC(kernel='rbf', C=5)\n",
    "model.fit(x_train,y_train)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the test dataset, evaluate the accuracy of the model. Again using the test dataset, compute the confusion matrix. What is \n",
    "#the most common mistake that the classifier makes?\n",
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_true = y_test, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test, y_pred = y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common mistake that the classifier makes?\n",
    "\n",
    "Interpretting 4s incorrectly as 7s. This happens 13 times in this data set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display all of the misclassified digits as images (title with: Predicted #, Actual #).\n",
    "#digits = load_digits()\n",
    "#print(digits.data.shape)\n",
    "counter = 0\n",
    "\n",
    "for i in range(y_pred.size):\n",
    "    if y_pred[i] != y_test[i]:\n",
    "        plt.gray()\n",
    "        plt.imshow(np.reshape(x_test[i],(8,8)), cmap='gray')\n",
    "        pridictedString = \"Predicted: \" + str(y_pred[i])\n",
    "        actualString = \"Actual: \" + str(y_test[i])\n",
    "        plt.xlabel(actualString, fontsize=10)\n",
    "        plt.ylabel(pridictedString, fontsize=10)\n",
    "        plt.show()\n",
    "        counter+= 1\n",
    "print(\"The total number of incorrect ones is: \" + str(counter))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the 'cross_val_score' function, evaluate the accuracy of the SVM for different values of the parameter C: .5 to 5 (by .1) \n",
    "# note: here we are again using all 4 features of the data\n",
    "\n",
    "Cs = np.arange(.5, 5, .1)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    model = svm.SVC(kernel='rbf',gamma='scale', C = C)\n",
    "    scores = cross_val_score(estimator = model, X = X, y = y, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and then 10-50 (by 20). What is the best value? (Graph results)\n",
    "# note: here we are again using all 4 features of the data\n",
    "Cs = np.arange(10, 50, 20)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    model = svm.SVC(kernel='rbf',gamma='scale', C = C)\n",
    "    scores = cross_val_score(estimator = model, X = X, y = y, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best value?\n",
    "\n",
    "Around 2.8~ is the best value. After 2.8 it becomes overfit and decreases slightly and levels out at around 95%, so increasing the value has no affect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test the algorithm on the raw (non-scaled) data. What's your accuracy score?\n",
    "digits2 = load_digits()\n",
    "X2 = digits2.data\n",
    "y2 = digits2.target\n",
    "\n",
    "n_samples2, n_features2 = X2.shape\n",
    "n_digits2 = len( np.unique(digits2.target) )\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split( X2, y2, random_state=1, test_size=0.8 )\n",
    "\n",
    "model2 = svm.SVC(kernel='rbf', C=5)\n",
    "model2.fit(x_train2,y_train2)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_pred2 = model2.predict(x_test2)\n",
    "print(metrics.confusion_matrix(y_true = y_test2, y_pred = y_pred2))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test2, y_pred = y_pred2))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your accuracy score?\n",
    "0.97566 or 97.55%. It shouldn't be more accurate but for some reason it is. It is not scaled, so I'm unsure why it is more accurate. huh."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2: Prediction with K-nearest Neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat task 1.1 using k-nearest neighbors (k-NN). In part 1, use k=10. In part 5, find the best value of k\n",
    "\n",
    "digitsk = load_digits()\n",
    "Xk = scale( digitsk.data )\n",
    "yk = digitsk.target\n",
    "\n",
    "\n",
    "n_samplesk, n_featuresk = Xk.shape\n",
    "n_digitsk = len( np.unique(digitsk.target) )\n",
    "\n",
    "x_traink, x_testk, y_traink, y_testk = train_test_split( Xk, yk, random_state=1, test_size=0.8 )\n",
    "\n",
    "\n",
    "\n",
    "kmodel = KNeighborsClassifier(n_neighbors=10)\n",
    "kmodel.fit(x_traink, y_traink)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_predk = kmodel.predict(x_testk)\n",
    "print(metrics.confusion_matrix(y_true = y_testk, y_pred = y_predk))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_testk, y_pred = y_predk))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the most common mistake that the classifier makes?\n",
    "\n",
    "The most common mistake in this case is that 1 is being interpretted as a 9, this occurs 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "\n",
    "for i in range(y_predk.size):\n",
    "    if y_predk[i] != y_testk[i]:\n",
    "        plt.gray()\n",
    "        plt.imshow(np.reshape(x_testk[i],(8,8)), cmap='gray')\n",
    "        pridictedStringk = \"Predicted: \" + str(y_predk[i])\n",
    "        actualStringk = \"Actual: \" + str(y_testk[i])\n",
    "        plt.xlabel(actualStringk, fontsize=10)\n",
    "        plt.ylabel(pridictedStringk, fontsize=10)\n",
    "        plt.show()\n",
    "        counter+= 1\n",
    "print(\"The total number of incorrect ones is: \" + str(counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Csk = np.arange(1, 50, 1)\n",
    "Accuraciesk = np.zeros(Csk.shape[0])\n",
    "for i,K in enumerate(Csk): \n",
    "    modelk = KNeighborsClassifier(n_neighbors= K)\n",
    "    scoresk = cross_val_score(estimator = modelk, X = Xk, y = yk, cv=5, scoring='accuracy')    \n",
    "    Accuraciesk[i]  = scoresk.mean()\n",
    "        \n",
    "plt.plot(Csk,Accuraciesk)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best value?\n",
    "\n",
    "The best value is at 3~ with a max of 0.945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitsk2 = load_digits()\n",
    "Xk2 = digitsk2.data\n",
    "yk2 = digitsk2.target\n",
    "\n",
    "n_samplesk2, n_featuresk2 = Xk2.shape\n",
    "n_digitsk2 = len( np.unique(digitsk2.target) )\n",
    "\n",
    "x_traink2, x_testk2, y_traink2, y_testk2 = train_test_split( Xk2, yk2, random_state=1, test_size=0.8 )\n",
    "\n",
    "modelk2 = KNeighborsClassifier(n_neighbors=10)\n",
    "modelk2.fit(x_traink2, y_traink2)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_predk2 = modelk2.predict(x_testk2)\n",
    "print(metrics.confusion_matrix(y_true = y_testk2, y_pred = y_predk2))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_testk2, y_pred = y_predk2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's your accuracy score?\n",
    "The accuracy is 0.938 or 93.8%. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Popularity of online news"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1: Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('OnlineNewsPopularity.csv')\n",
    "shares = df[' shares'].to_numpy()\n",
    "\n",
    "df = df.drop(columns=['url', ' timedelta', ' shares'])\n",
    "\n",
    "X = df.to_numpy()\n",
    "\n",
    "Y = []\n",
    "\n",
    "median_share = np.median(shares)\n",
    "\n",
    "Y = np.where(shares > median_share, 1, 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First check to see if the values are reasonable. What are the min, median, and maximum number of shares?\n",
    "\n",
    "print(\"The minimum value in shares is: \" + str(shares.min()))\n",
    "print(\"The maximum value in shares is: \" + str(shares.max()))\n",
    "print(\"The median value of shares is: \" + str(median_share))\n",
    "\n",
    "#In each of the following tasks, make sure to show enough information to explain your results (graphs, images, etc, as necessary)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3: Classification Using k-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop a k-NN classification model for the data. Use cross validation to choose the best value of k. \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( X, Y, random_state=1, test_size=0.8 )\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=10)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_true = y_test, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test, y_pred = y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.arange(1, 50, 1)\n",
    "Accuracies = np.zeros(C.shape[0])\n",
    "for i,K in enumerate(C): \n",
    "    model = KNeighborsClassifier(n_neighbors= K)\n",
    "    scores = cross_val_score(estimator = model, X = X, y = Y, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(C,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C[Accuracies.argmax()])\n",
    "print(Accuracies.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running again with the best K value\n",
    "model = KNeighborsClassifier(n_neighbors=38)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_pred = model.predict(x_test)\n",
    "print(metrics.confusion_matrix(y_true = y_test, y_pred = y_pred))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_test, y_pred = y_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best accuracy you can obtain on the test data?\n",
    "The best value for K is 38 based on this data. \n",
    "BUT the highest accuracy is 0.561 which is a terrible accuracy. using the best number of neighbors barely improved the accuracy. with a K of 10, the accuracy was 54.9% and increasing the K to 38 improved the accuracy by less than 2 full percentage points\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 Classification using SVM\n",
    "\n",
    "Develop a support vector machine classification model for the data.\n",
    "\n",
    "SVM is computationally expensive, so start by using only a fraction of the data, say 5,000 articles.\n",
    "Experiment with different Cs. Which is the best value for C?\n",
    "Note that it takes multiple minutes per value of C to run on the whole dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test the algorithm on the raw (non-scaled) data. What's your accuracy score?\n",
    "\n",
    "x_trainSVM, x_testSVM, y_trainSVM, y_testSVM = train_test_split( X, Y, random_state=1, test_size=0.8 )\n",
    "\n",
    "pt2SVM = svm.SVC(kernel='rbf', C=5)\n",
    "pt2SVM.fit(x_trainSVM, y_trainSVM)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_predSVM = pt2SVM.predict(x_testSVM)\n",
    "print(metrics.confusion_matrix(y_true = y_testSVM, y_pred = y_predSVM))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_testSVM, y_pred = y_predSVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol_random_nums = np.random.choice(X.shape[0], 5000, replace=False)\n",
    "rand5000X = X[smol_random_nums]\n",
    "rand5000y = Y[smol_random_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.arange(1000, 30000, 1000)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    model = svm.SVC(kernel='rbf',gamma='scale', C = C)\n",
    "    scores = cross_val_score(estimator = model, X = rand5000X, y = rand5000y, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.arange(20000, 23000, 200)\n",
    "Accuracies = np.zeros(Cs.shape[0])\n",
    "for i,C in enumerate(Cs): \n",
    "    model = svm.SVC(kernel='rbf',gamma='scale', C = C)\n",
    "    scores = cross_val_score(estimator = model, X = rand5000X, y = rand5000y, cv=5, scoring='accuracy')    \n",
    "    Accuracies[i]  = scores.mean()\n",
    "        \n",
    "plt.plot(Cs,Accuracies)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Cs[Accuracies.argmax()])\n",
    "print(Accuracies.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainSVM, x_testSVM, y_trainSVM, y_testSVM = train_test_split( X, Y, random_state=1, test_size=0.8 )\n",
    "\n",
    "pt2SVM2 = svm.SVC(kernel='rbf', C=21000)\n",
    "pt2SVM2.fit(x_trainSVM, y_trainSVM)\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "y_predSVM = pt2SVM2.predict(x_testSVM)\n",
    "print(metrics.confusion_matrix(y_true = y_testSVM, y_pred = y_predSVM))\n",
    "\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = y_testSVM, y_pred = y_predSVM))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the best value for C?\n",
    "The best value for C was 21000. this raised the accuracy from 55%~ to 60.1%~! nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dav's comment in slack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5 Classification using decision trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Develop a decision tree classification model for the data.\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(X, Y, random_state=1, test_size=0.2)\n",
    "decisionTree = tree.DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "tree.plot_tree(decisionTree, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = np.arange(2, 20, 1)\n",
    "depth_Accuracies = np.zeros(depths.shape[0])\n",
    "for i,depth in enumerate(depths):\n",
    "\n",
    "    decisionTree = tree.DecisionTreeClassifier(max_depth=depth)\n",
    "\n",
    "    decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "    y_pred_train = decisionTree.predict(XTrain)\n",
    "    #print(\"depth= \" + str(depth))\n",
    "    #print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "    y_pred = decisionTree.predict(XTest)\n",
    "    #print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "\n",
    "    scores = cross_val_score(estimator = decisionTree, X = X, y = Y, cv=5, scoring='accuracy')    \n",
    "    depth_Accuracies[i]  = scores.mean()\n",
    "    #print(\"scores mean: \" + str(scores.mean()))\n",
    "\n",
    "plt.plot(depths,depth_Accuracies)\n",
    "plt.show()\n",
    "print(depths[depth_Accuracies.argmax()])\n",
    "print(depth_Accuracies.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use cross validation to choose good values of the max tree depth (max_depth) and minimum samples split (min_samples_split).\n",
    "\n",
    "min_samples = np.arange(2, 300, 5)\n",
    "sample_Accuracies = np.zeros(min_samples.shape[0])\n",
    "for i,sample in enumerate(min_samples):\n",
    "\n",
    "    decisionTree = tree.DecisionTreeClassifier(max_depth=6, min_samples_split=sample)\n",
    "\n",
    "    decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "    y_pred_train = decisionTree.predict(XTrain)\n",
    "    #print(\"samples= \" + str(min_samples))\n",
    "    #print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "    y_pred = decisionTree.predict(XTest)\n",
    "    #print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n",
    "\n",
    "    scores = cross_val_score(estimator = decisionTree, X = X, y = Y, cv=5, scoring='accuracy')    \n",
    "    sample_Accuracies[i]  = scores.mean()\n",
    "    #print(\"scores mean: \" + str(scores.mean()))\n",
    "\n",
    "plt.plot(min_samples,sample_Accuracies)\n",
    "plt.show()\n",
    "print(min_samples[sample_Accuracies.argmax()])\n",
    "print(sample_Accuracies.max())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best depth for this tree is 6 which gives us a 61% accuracy score!!\n",
    "The best minimum samples split is technically 262 in this instance, but the impact from the minimum samples split is so small that it is negligable. different values result in a change of smaller than a percentage point, so the impact of best depth is significantly more impactful that the min sample split\n",
    "\n",
    "The accuracy with both of these values together is 61%~\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6 Describe your findings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which method (k-NN, SVM, Decision Tree) worked best?\n",
    "\n",
    "Decision tree worked best with an accuracy value of 61%. This would probably be improved by including some unknown confounding factors\n",
    "\n",
    "\n",
    "How did different parameters influence the accuracy?\n",
    "\n",
    "the minimum samples split had hardly any impact in a deciion tree. The depth of a tree was pretty impactful with almost 4% of improved accuracy. The C value in SVM was moderately impactful. The best K value for KNN resulted in only a 1% improvement in accuracy\n",
    "\n",
    "Which model is easiest to interpret?\n",
    "\n",
    "Decision tree is the easiest to visually interpret, I think KNN is the most difficult to conceptually understand\n",
    "\n",
    "How would you interpret your results?\n",
    "\n",
    "The most impactful values on the number of shares are num_keywords, data_channel_is_entertainment, data_channel_is_tech, and is_weekend. I would also interpret this as not having the best model. there are definitley other factors not being taken into consideration in these models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
