{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitney Kenner\n",
    "u0777962\n",
    "HW6\n",
    "CS 6017\n",
    "7-17-23\n",
    "Character Classification using CNNs with PyTorch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Acquisition + Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"fonts/TIMES.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareDF(dataframe):\n",
    "    df = dataframe.drop(columns=['font', 'fontVariant', 'strength', 'italic', 'orientation', 'm_top', 'm_left', 'originalH', 'originalW', 'h', 'w'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepareDF(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize_X_and_y(dataframe):\n",
    "    \n",
    "    df1 = dataframe[dataframe['m_label'] > 33]\n",
    "    df = df1[df1['m_label'] < 126]\n",
    "    yArray = np.array(df['m_label'])\n",
    "\n",
    "    temp_dataframe = df.drop(columns=['m_label'])\n",
    "    xArray = np.zeros((len(temp_dataframe), 20, 20))\n",
    "    for row in range(0, temp_dataframe.shape[0]):\n",
    "        for i in range(20):\n",
    "            for j in range(20):\n",
    "                string = 'r' + str(i) + 'c' + str(j)\n",
    "                xArray[row, i, j] = temp_dataframe.iloc[row].loc[string] /255\n",
    "    \n",
    "    \n",
    "    return xArray, yArray\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xVals, yVals = normalize_X_and_y(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xVals[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAsciiDictionaries(yValues):\n",
    "    unique_vals = set(yValues)\n",
    "    setSize = len(unique_vals)\n",
    "    index_to_ascii = {}\n",
    "    ascii_to_index = {}\n",
    "    index = 0\n",
    "    for val in unique_vals:\n",
    "        ascii_to_index[val] = index\n",
    "        index_to_ascii[index] = val\n",
    "        index+=1\n",
    "\n",
    "    print(index_to_ascii)\n",
    "    print(ascii_to_index)\n",
    "    return ascii_to_index, index_to_ascii, setSize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asciiToIndex, indexToAscii, setSize = getAsciiDictionaries(yVals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToIndex(yValues, ascii_to_index):\n",
    "    for i in range(len(yValues)):\n",
    "        yValues[i] = ascii_to_index[yValues[i]]\n",
    "\n",
    "    return yValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yVals = convertToIndex(yVals, asciiToIndex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a PyTorch Network\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the PyTorch library, like we've seen in class, to build/train our network. Check out the notebooks we've made in class or the official documentation/tutorials.\n",
    "\n",
    "To start with, we're going to use a model very similar to the MNIST CNN we used in class. It will consist of:\n",
    "\n",
    "a Convolution2D layer with ReLU activations\n",
    "a max pooling layer\n",
    "another convolution layer\n",
    "another max pooling layer\n",
    "a dense layer with relu activation\n",
    "a dense layer\n",
    "Compile and train your network like we did in class. You'll probably have to use the np.reshape() function on your data to make PyTorch happy. I reshaped my X values like np.reshape(Xs, (-1, 1, 20, 20)) to get them in the right format.\n",
    "\n",
    "For training, you'll want to check out torch.utils.data.DataLoader which can take a TensorDataset so you can iterate over batches like we did in class for the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score, KFold\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xVals.data.shape)\n",
    "print(setSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize= (10, 10) )\n",
    "\n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( xVals[ii, :, :], cmap='Greys',interpolation='none' )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = np.reshape(xVals, (-1, 1, 20, 20))\n",
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into train/test\n",
    "\n",
    "print(\"Xs: \", len(Xs))\n",
    "print(\"Ys: \", len(yVals))\n",
    "\n",
    "x_tensor = torch.tensor(Xs, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(yVals)\n",
    "\n",
    "print(len(x_tensor))\n",
    "print(len(y_tensor))\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_tensor, y_tensor, random_state=1, test_size=0.9)\n",
    "\n",
    "training_data = TensorDataset(x_train, y_train)\n",
    "testing_data = TensorDataset(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        c1Out = 6 # convolution layer 1 will output 6 \"images\": one for each filter it trains\n",
    "        c2Out = 16 # similarly for the 2nd convolution layer\n",
    "        self.conv1 = nn.Conv2d( 1, c1Out, 3 ) # 1-D input, c1Out outputs, filter size 3x3 pixels\n",
    "        \n",
    "        # (28-2) x (28 -2) x c1Out outputs  # \"-2\" because 3x3 mask loses the 1st/last row/column\n",
    "        \n",
    "        self.pool = nn.MaxPool2d( 2, 2 ) # down sample 2x2 blocks to 1 value\n",
    "        \n",
    "        # 13*13*c1Out\n",
    "        \n",
    "        self.conv2 = nn.Conv2d( c1Out, c2Out, 3 ) # Inputs comes from conv1, specify our #outputs, use 3x3 blocks again\n",
    "        \n",
    "        # (13-2)*(13 -2)*c2Out\n",
    "        # pool again\n",
    "        # (11/2)*(11/2)*c2Out = 5x5 x c2Out\n",
    "        \n",
    "        #this is tricky.  The convolutions each shave 1 pixel off around the border, and then the\n",
    "        #max pools reduce the number of pixels by 4\n",
    "        self.pooledOutputSize = c2Out * 3 * 3 # 16 outputs per image whose size has been reduced\n",
    "        self.fc1 = nn.Linear( self.pooledOutputSize, 120 )\n",
    "        self.fc2 = nn.Linear( 120, 84 )\n",
    "        self.fc3 = nn.Linear( 84, setSize ) # 10 outputs at the end\n",
    "\n",
    "    ################################################################################\n",
    "    # Take an image (or images) and run it through all stages of the net:\n",
    "    #    \n",
    "    def forward( self, x ): # \"batch\" of images\n",
    "        # x is 4D tensor:  (batch size, width, height, #channels (1, grayscale image))\n",
    "        # after conv1:  (batch size, width adjusted, height adjusted, conv1 # outputs)\n",
    "        # after max pool: (batch size, width/2, height/2, conv1 # outputs)\n",
    "        \n",
    "        # print(x.shape) # During creation / debugging, getting the shape of layers correct is challenging... so display them.\n",
    "        #x = F.relu(self.conv1(x))\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        # print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Split into 2 lines above\n",
    "        #x = self.pool(F.relu(self.conv1(x)))  #apply convolution filter, then run it through relu activation function\n",
    "        x = self.pool(F.relu(self.conv2(x))) #ditto\n",
    "        #print(x.shape) #uncomment to see the size of this layer.  It helped me figure out what pooledOutputSize shoudl be\n",
    "\n",
    "        # Flatten: turn the 5x5xc2Out array into a single 1xN array.  The dense layers expect a 1D thing\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # x = x.view(x.shape[0], -1)  #equivalent ways of reshaping the data to be 1D\n",
    "        # x = x.view(batch_size( x.shape[0]) , -1)\n",
    "        x = F.relu(self.fc1(x)) #apply dense layer 1\n",
    "        x = F.relu(self.fc2(x)) #and dense layer 2, using ReLU activation\n",
    "        x = self.fc3(x) #final dense layer.  No activation function on this\n",
    "        return x\n",
    "    \n",
    "    #compute the output size after our convolution layers\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, epochs, training_data ): # One epoch uses the entire training set (one batch at a time) - 60,000 images in this case\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss() # this is a way of measuring error (loss) for classification that takes the\n",
    "                                      # \"confidence\" of a prediction into account.  High confidence, correct predictions are low cost, \n",
    "                                      # high confidence, wrong predictions are high cost, medium confidence predictions have cost\n",
    "\n",
    "    # use the ADAM optimizer to find the best weights\n",
    "    optimizer = optim.Adam( model.parameters(), lr= 1e-4 ) \n",
    "    \n",
    "    #this loads data and gets it in the right format for us\n",
    "    trainloader = torch.utils.data.DataLoader( training_data, batch_size=8,\n",
    "                                               shuffle=True, num_workers=0 )\n",
    "\n",
    "    for epoch in range( epochs ): # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate( trainloader, 0 ):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            inputs = x_tensor\n",
    "            labels = y_tensor\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs) #predict the output with some training data\n",
    "            loss = criterion(outputs, labels) #see how well we did\n",
    "\n",
    "            loss.backward() #see how to change the weights to do better\n",
    "            optimizer.step() #and actually change the weights\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "def evaluate( model, testing_data, iToA):  \n",
    "    #load some test data\n",
    "    testloader = torch.utils.data.DataLoader( testing_data, batch_size=1,\n",
    "                                              shuffle=True, num_workers=0 )\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad(): # <- Since we are not training, the model does not need to calculate gradients\n",
    "        count = 0\n",
    "        for data in testloader:\n",
    "            \n",
    "            images, labels = data\n",
    "            outputs = model( images )\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            if predicted != labels:\n",
    "                ascii = chr(iToA[predicted.item()])\n",
    "\n",
    "                plt.subplot(5,10, count+1)\n",
    "                \n",
    "                #plt.ylabel(\"actual: \", labels.item())\n",
    "                plt.imshow(images[0,0])\n",
    "                plt.title(ascii)\n",
    "                plt.axis('off')\n",
    "                \n",
    "                count+=1\n",
    "                if count >=50:\n",
    "                    break\n",
    "            #plt.subplot.figsize=(30,30)\n",
    "\n",
    "    # Just do a coarse evaluation... how many did we predict correcly?\n",
    "    print( 'Accuracy of the network on the 10000 test images: %d %%' % ( 100 * correct / total) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: On my home (older PC), this takes 3-ish minutes to run...\n",
    "print( \"Training...\" )\n",
    "train( net, 15, training_data )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Exploration and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print( \"Evaluating...\" )\n",
    "evaluate( net, testing_data, indexToAscii )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What is its accuracy?  \n",
    " 50% in this instance. I can also increase the number of epochs and it increases substantially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and train a different network topology (add more convolution layers, experiment with normalization (batch normalization or dropout), \n",
    "#explore other types/sizes of layer). Try to find a topology that works better than the one described above.\n",
    "class NewNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NewNet, self).__init__()\n",
    "        c1Out = 16 # convolution layer 1 will output 6 \"images\": one for each filter it trains\n",
    "        c2Out = 32 # similarly for the 2nd convolution layer\n",
    "        c3Out = 32\n",
    "        self.conv1 = nn.Conv2d( 1, c1Out, 3 ) # 1-D input, c1Out outputs, filter size 3x3 pixels\n",
    "        \n",
    "        # (28-2) x (28 -2) x c1Out outputs  # \"-2\" because 3x3 mask loses the 1st/last row/column\n",
    "        \n",
    "        self.pool = nn.MaxPool2d( 2, 2 ) # down sample 2x2 blocks to 1 value\n",
    "        \n",
    "        # 13*13*c1Out\n",
    "        \n",
    "        self.conv2 = nn.Conv2d( c1Out, c2Out, 3 ) # Inputs comes from conv1, specify our #outputs, use 3x3 blocks again\n",
    "\n",
    "        self.conv3 = nn.Conv3d(c2Out, c3Out, 3)\n",
    "        \n",
    "        # (13-2)*(13 -2)*c2Out\n",
    "        # pool again\n",
    "        # (11/2)*(11/2)*c2Out = 5x5 x c2Out\n",
    "        \n",
    "        #this is tricky.  The convolutions each shave 1 pixel off around the border, and then the\n",
    "        #max pools reduce the number of pixels by 4\n",
    "        self.pooledOutputSize = c2Out * 3 * 3 # 16 outputs per image whose size has been reduced\n",
    "        self.fc1 = nn.Linear( self.pooledOutputSize, 120 )\n",
    "        self.fc2 = nn.Linear( 120, 84 )\n",
    "        self.fc3 = nn.Linear( 84, setSize ) # 10 outputs at the end\n",
    "\n",
    "    ################################################################################\n",
    "    # Take an image (or images) and run it through all stages of the net:\n",
    "    #    \n",
    "    def forward( self, x ): # \"batch\" of images\n",
    "        # x is 4D tensor:  (batch size, width, height, #channels (1, grayscale image))\n",
    "        # after conv1:  (batch size, width adjusted, height adjusted, conv1 # outputs)\n",
    "        # after max pool: (batch size, width/2, height/2, conv1 # outputs)\n",
    "        \n",
    "        # print(x.shape) # During creation / debugging, getting the shape of layers correct is challenging... so display them.\n",
    "        #x = F.relu(self.conv1(x))\n",
    "        x = self.conv1(x)\n",
    "        # print(x.shape)\n",
    "        x = F.relu(x)\n",
    "        # print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        # Split into 2 lines above\n",
    "        #x = self.pool(F.relu(self.conv1(x)))  #apply convolution filter, then run it through relu activation function\n",
    "        x = self.pool(F.relu(self.conv2(x))) #ditto\n",
    "        #print(x.shape) #uncomment to see the size of this layer.  It helped me figure out what pooledOutputSize shoudl be\n",
    "\n",
    "        # Flatten: turn the 5x5xc2Out array into a single 1xN array.  The dense layers expect a 1D thing\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # x = x.view(x.shape[0], -1)  #equivalent ways of reshaping the data to be 1D\n",
    "        # x = x.view(batch_size( x.shape[0]) , -1)\n",
    "        x = F.relu(self.fc1(x)) #apply dense layer 1\n",
    "        x = F.relu(self.fc2(x)) #and dense layer 2, using ReLU activation\n",
    "        x = self.fc3(x) #final dense layer.  No activation function on this\n",
    "        return x\n",
    "    \n",
    "    #compute the output size after our convolution layers\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "newnet = NewNet()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: On my home (older PC), this takes 3-ish minutes to run...\n",
    "print( \"Training...\" )\n",
    "train( newnet, 15, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Evaluating...\" )\n",
    "evaluate( newnet, testing_data, indexToAscii )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the accuracy of your network with character inputs from a DIFFERENT font set. How does it perform?\n",
    "df2 = pd.read_csv(\"fonts/ERAS.csv\")\n",
    "df2 = prepareDF(df2)\n",
    "xValSitka, yValSitka = normalize_X_and_y(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ascii_to_index_sitka, index_to_ascii_sitka, setSize = getAsciiDictionaries(yValSitka)\n",
    "Ys = convertToIndex(yValSitka, ascii_to_index_sitka)\n",
    "\n",
    "\n",
    "plt.figure( figsize= (10, 10) )\n",
    "\n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( xValSitka[ii, :, :], cmap='Greys',interpolation='none' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Xs_sitka = np.reshape(xValSitka, (-1, 1, 20, 20))\n",
    "\n",
    "x_tensor_sitka = torch.tensor(Xs_sitka, dtype=torch.float32)\n",
    "y_tensor_sitka = torch.tensor(Ys)\n",
    "\n",
    "print(len(x_tensor_sitka))\n",
    "print(len(y_tensor_sitka))\n",
    "\n",
    "\n",
    "x_train_sitka, x_test_sitka, y_train_sitka, y_test_sitka = train_test_split(x_tensor_sitka, y_tensor_sitka, random_state=1, test_size=0.9)\n",
    "\n",
    "training_data_sitka = TensorDataset(x_train_sitka, y_train_sitka)\n",
    "testing_data_sitka = TensorDataset(x_test_sitka, y_test_sitka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Note: On my home (older PC), this takes 3-ish minutes to run...\n",
    "# print( \"Training...\" )\n",
    "# train( net, 22, training_data_sitka )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Evaluating...\" )\n",
    "evaluate( net, testing_data_sitka, index_to_ascii_sitka )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it perform?\n",
    "It performs terribly, somewhere between 1 and 7% when everything is re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Training...\" )\n",
    "train( net, 15, training_data_sitka)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Evaluating...\" )\n",
    "evaluate( net, testing_data_sitka, index_to_ascii_sitka)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does your accuracy compare to the 1-font case? What accuracy do you see when testing with inputs from a font you didn't train on?  \n",
    "it's much worse, the fonts I picked are quite different. my 2nd font also has a smaller sample size than the first one, so less training data  \n",
    "most times I got 1% or 0% when testing with inputs I didn't train on, which makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#third font:\n",
    "\n",
    "df_elephant = pd.read_csv(\"fonts/SERIF.csv\")\n",
    "df3 = prepareDF(df_elephant)\n",
    "xValEl, yValEl = normalize_X_and_y(df3)\n",
    "\n",
    "ascii_to_index_elephant, index_to_ascii_elephant, setSize = getAsciiDictionaries(yValEl)\n",
    "Ys_el = convertToIndex(yValEl, ascii_to_index_elephant)\n",
    "\n",
    "\n",
    "plt.figure( figsize= (10, 10) )\n",
    "\n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( xValEl[ii, :, :], cmap='Greys',interpolation='none' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Xs_el = np.reshape(xValEl, (-1, 1, 20, 20))\n",
    "\n",
    "x_tensor_el = torch.tensor(Xs_el, dtype=torch.float32)\n",
    "y_tensor_el = torch.tensor(Ys_el)\n",
    "\n",
    "print(len(x_tensor_el))\n",
    "print(len(y_tensor_el))\n",
    "\n",
    "\n",
    "x_train_el, x_test_el, y_train_el, y_test_el = train_test_split(x_tensor_el, y_tensor_el, random_state=1, test_size=0.9)\n",
    "\n",
    "training_data_el = TensorDataset(x_train_el, y_train_el)\n",
    "testing_data_el = TensorDataset(x_test_el, y_test_el)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Evaluating...\" )\n",
    "evaluate( net, testing_data_el, index_to_ascii_elephant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice any patterns? The network only produces the relative probabilities that the input is any of the possible characters. Can you find examples where the network is unsure of the result?  \n",
    "my models seem to have a really hard time with expecting numbers and symbols. I removed all the unicode characters outside of the ascii range, so those are not part of the input, but it often seems to misclassify letters as numbers, I wonder if the sample size of numbers is smaller than other characters and therefor my model is bad at predicting them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we'll build and train a neural network - an autoencoder - for a different task: denoising images.\n",
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        \n",
    "        self.encodedSize = 32\n",
    "        \n",
    "        self.c1Out = 8 # filters from first conv layer\n",
    "        self.c2Out = 8 # filters from 2nd conv layer\n",
    "        \n",
    "        #the padding here puts a \"border\" of 0s around the image, so that convolution layers don't \"shrink\" the image\n",
    "        \n",
    "        self.cv1 = nn.Conv2d(1, self.c1Out, 3, padding=1) #stick with 3x3 filters\n",
    "        #output is 8x 28x28 images\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.cv2 = nn.Conv2d(self.c1Out, self.c2Out, 3, padding=1)\n",
    "        #reuse pool here\n",
    "        \n",
    "        self.downscaledSize = 20//4 #we add padding, so the conv2d layers don't change the size, just the max pools\n",
    "        self.flattenedSize = self.downscaledSize*self.downscaledSize*self.c2Out\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattenedSize, 64)\n",
    "        self.fc2 = nn.Linear(64, self.encodedSize) #scale down to 64 features\n",
    "\n",
    "        #now we're encoded, so go define decoding pieces\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.encodedSize, 64) #scale down to 64 features\n",
    "        self.fc4 = nn.Linear(64, self.flattenedSize)\n",
    "        \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        # the padding is very important here so we don't have to guess a \"frame\" of pixels around the image\n",
    "        self.cv3 = nn.Conv2d(self.c2Out, self.c1Out, 3, padding=1)\n",
    "        # apply upsample again\n",
    "        self.cv4 = nn.Conv2d(self.c1Out, 1, 3, padding=1)\n",
    "        \n",
    "        \n",
    "    def compress(self, x):\n",
    "        x = self.cv1(x)\n",
    "        #print(\"shape after cv1\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"shape after pool1\", x.shape)\n",
    "        x = self.cv2(x)\n",
    "        #print(\"after cv2\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"after pool 2\", x.shape)\n",
    "        x = x.view(-1, self.flattenedSize)\n",
    "        #print(\"flattened shape\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #now we have a low-d representation of our data.  If we were doing compression, we'd store this\n",
    "        return x\n",
    "    \n",
    "    def decompress(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, self.c2Out, self.downscaledSize, self.downscaledSize)\n",
    "        #print(\"unflattened shape\", x.shape)\n",
    "        x = self.upsample(x)\n",
    "        #print(\"upsample\", x.shape)\n",
    "        x = F.relu(self.cv3(x))\n",
    "        #print(x.shape, \"after cv3\")\n",
    "        x = self.cv4(self.upsample(x))\n",
    "        #print(x.shape, \"after both upsamples\")\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "cnnNet = CnnNet() # treat these as just 28 D vectors\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def trainCNN(model, epochs, train_data):\n",
    "    # create an optimizer object\n",
    "    # Adam optimizer with learning rate 1e-3\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            #same as yesterday, except we're not even looking at the labels!\n",
    "            # since we're not using a CNN, we need to \"flatten\" the input images\n",
    "            batch_features = data[0]\n",
    "        \n",
    "            # reset the gradients back to zero\n",
    "            # PyTorch accumulates gradients on subsequent backward passes\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute reconstructions\n",
    "            outputs = model(batch_features)\n",
    "            #print(batch_features.shape)\n",
    "            #print(outputs.shape)\n",
    "            # compute training reconstruction loss\n",
    "            # again, same idea as yesterday, but we're measuring the error slightly differently\n",
    "            # how well does the reconstructed image match the input image?\n",
    "            train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "            # compute accumulated gradients\n",
    "            train_loss.backward()\n",
    "        \n",
    "            # perform parameter update based on current gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            loss += train_loss.item()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += train_loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.8f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "        # compute the epoch training loss\n",
    "        loss = loss / len(train_loader)\n",
    "    \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}\".format(epoch + 1, epochs, loss))\n",
    "\n",
    "def evaluateCNN(model, test_data):\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images = data[0]\n",
    "            outputs = model(images)\n",
    "            test_loss = criterion(outputs, images)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "    print(\"overall loss: \", total_loss)\n",
    "\n",
    "\n",
    "\n",
    "def drawComparisonsCNN(model, test_data):\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=8, shuffle=True, num_workers=0)\n",
    "    plt.figure(figsize=(20, 25))\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 8: break\n",
    "        images = batch[0]\n",
    "        #print(images.shape)\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(images)\n",
    "            for j in range(len(images)):\n",
    "                #draw the original image\n",
    "                ax = plt.subplot(32, 16, i*32 + j + 1)\n",
    "                plt.imshow(images[j].reshape((20,20)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "                #and the reconstructed version in the next row\n",
    "                ax = plt.subplot(32, 16, i*32 + j + 1 + 16)\n",
    "                plt.imshow(reconstructed[j].reshape((20,20)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pre_sound = pd.read_csv(\"fonts/SERIF.csv\")\n",
    "df_pre_sound = prepareDF(df_pre_sound)\n",
    "xValSound, yValSound = normalize_X_and_y(df_pre_sound)\n",
    "\n",
    "\n",
    "\n",
    "ascii_to_index_sound, index_to_ascii_sound, setSize = getAsciiDictionaries(yValSound)\n",
    "Ys = convertToIndex(yValSound, ascii_to_index_sound)\n",
    "\n",
    "\n",
    "#add sound\n",
    "noise_to_add = np.random.normal(0,.1,xValSound.shape)\n",
    "xValSound = xValSound + noise_to_add\n",
    "\n",
    "\n",
    "plt.figure( figsize= (10, 10) )\n",
    "\n",
    "for ii in np.arange( 25 ):\n",
    "    plt.subplot( 5, 5, ii+1 )\n",
    "    plt.imshow( xValSound[ii, :, :], cmap='Greys',interpolation='none' )\n",
    "\n",
    "plt.show()\n",
    "\n",
    "Xs_sound = np.reshape(xValSound, (-1, 1, 20, 20))\n",
    "\n",
    "x_tensor_sound = torch.tensor(Xs_sound, dtype=torch.float32)\n",
    "y_tensor_sound = torch.tensor(Ys)\n",
    "\n",
    "print(len(x_tensor_sound))\n",
    "print(len(y_tensor_sound))\n",
    "\n",
    "\n",
    "x_train_sound, x_test_sound, y_train_sound, y_test_sound = train_test_split(x_tensor_sound, y_tensor_sound, random_state=1, test_size=0.9)\n",
    "\n",
    "training_data_sound = TensorDataset(x_train_sound, y_train_sound)\n",
    "testing_data_sound = TensorDataset(x_test_sound, y_test_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCNN(cnnNet, 15, training_data_sound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluateCNN(cnnNet, testing_data_sound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a plot showing the noisy and denoised versions of some inputs to verify that your denoiser had the desired effect.  Discuss your results.\n",
    "\n",
    "drawComparisonsCNN(cnnNet, testing_data_sound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
