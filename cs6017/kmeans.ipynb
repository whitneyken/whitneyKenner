{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means Clustering\n",
    "\n",
    "Adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we will introduce clustering, an **unsupervised** learning technique. We'll cover the following topics:\n",
    "* supervised vs. unsupervised learning\n",
    "* clustering \n",
    "* k-means method\n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 10.1 and 10.3. [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* J. Grus, Data Science from Scratch, Ch. 19\n",
    "* [scikit-learn documentation on clustering](http://scikit-learn.org/stable/modules/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised vs unsupervised learning\n",
    "\n",
    "In this course, we've already discussed regression and classification, which are examples of [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) tasks. In supervised learning, you \"learn\" a function, $y = f(x)$, that maps an input, $x$, to an output or label, $y$, based on example input-output data. In regression, $x$ is a feature vector and $y$ is a real-valued number. In classification, $x$ is a feature vector and $y$ is a discrete variable, e.g., an integer or class. \n",
    "\n",
    "One difficulty of supervised learning is that labels can be expensive or impossible to obtain!\n",
    "\n",
    "In [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning), one tries to learn hidden structure from \"unlabeled\" data. Examples of unsupervised learning tasks include \n",
    "* Clustering\n",
    "* Dimensionality reduction\n",
    "* Density estimation \n",
    "* Anomaly detection\n",
    "* ... \n",
    "\n",
    "In this lecture, we'll focus on clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "[Clustering](https://en.wikipedia.org/wiki/Cluster_analysis) is the task of discovering unknown subgroups in data, which we call *clusters*.  In other words, the **goal** is to partition the datset into clusters where ‘similar’ items are in the same cluster and ‘dissimilar’ items are in different clusters. \n",
    "\n",
    "**Examples:**\n",
    "* Social Network Analysis: Clustering can be used to find communities\n",
    "* Ecology: cluster organisms that share attributes into species, genus, etc...\n",
    "* Handwritten digits where the digits are unknown\n",
    "\n",
    "**Question:** What is the difference between *Clustering* and *Classificaiton*? \n",
    "\n",
    "There are several **challenges** to clustering:\n",
    "* We must make sense of what it means for two items in the dataset to be *similar* to one another. In other words, we need to develop a \"distance\" or a \"similarity\" function and there are many different choices: Euclidean distance, Pearson correlation, Manhattan distance, weighted distances, Jaccard coefficient, ... \n",
    "* Since the data is unlabelled, there is no \"ground truth\" to compare to. It is difficult to evaluate the accuracy of clustering methods. In particular, we can't simply do cross validation. \n",
    "\n",
    "There are several **methods** for clustering. We'll discuss: \n",
    "* k-means clustering\n",
    "* hierarchical clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The k-means clustering method\n",
    "\n",
    "**Data:**  A collection of points $\\{x_i\\}$, for $i = 1,\\ldots n$, where $x_i\\in \\mathbb R^d$. \n",
    "\n",
    "In [k-means clustering](https://en.wikipedia.org/wiki/K-means_clustering), one tries to find $k$ *centers*, $\\{\\mu_\\ell\\}$, $\\ell = 1,\\ldots k$, and assign each point $x$ to a *cluster* $C_\\ell$ with center $\\mu_\\ell$, as to minimize the *total intra-cluster distance* \n",
    "$$\n",
    "\\arg\\min_{\\mu, C} \\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "Here, $\\mu_\\ell$ is the mean of points in $C_\\ell$. The total intra-cluster distance is the total squared Euclidean distance from each point to the center of its cluster. It's a measure of the varaince or internal coherence of the clusters. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lloyd's Algorithm\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\mu, C} \\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "\n",
    "In practice, k-Means is implemented using [Lloyd's algorithm](https://en.wikipedia.org/wiki/Lloyd%27s_algorithm). \n",
    "\n",
    "**Input:** set of points $x_1,\\ldots, x_n$ and an integer $k$ (# clusters)\n",
    "\n",
    "Pick $k$ starting points as centers $\\mu_1, \\ldots, \\mu_k$.\n",
    "\n",
    "** while** not converged:\n",
    "1. Assign each point $x_i$, to the cluster $C_\\ell$ with closest center $\\mu_\\ell$. \n",
    "2. For each cluster $C_\\ell$, compute a new center, $\\mu_\\ell$, by taking the mean of all $x_i$ assigned to cluster $C_\\ell$, i.e., \n",
    "$$\n",
    "\\mu_\\ell = \\frac{1}{|C_\\ell|}\\sum_{x_i \\in C_\\ell} x_i\n",
    "$$\n",
    "\n",
    "### Comments\n",
    "1. How do we assign each point $x_i$, to the closest center, $C_\\ell$? \n",
    " - For every $\\mu_\\ell$, we calculate the distance $d(x_i, \\mu_\\ell)$. \n",
    " - Then we assign $x_i$ to cluster $C_\\ell$ with the smallest distance.\n",
    " \n",
    "+ How do we pick $k$ starting points as centers $\\mu_1, \\ldots \\mu_k$? Ideas:\n",
    " - randomly\n",
    " - points that are far apart? \n",
    " - manually? \n",
    " \n",
    "+  What does *not converged* mean? Ideas:\n",
    " - no point has changed cluster\n",
    " - distance between old and new centroid below threshold\n",
    " - number of max iterations reached\n",
    "+ How to choose the parameter $k$? Ideas:\n",
    " - visual comparison\n",
    " - compare the value of the total intra-cluster distance for different values of $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## k-means method demo\n",
    "There is a nice visualization of the k-means method by Naftali Harris here:\n",
    "\n",
    "[visualizing-k-means-clustering](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) \n",
    "        \n",
    "![k-means clustering](k-means-fig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance and properties of k-means\n",
    "\n",
    "* The performance is $O(n*k*d*i)$ where \n",
    " - n is the number of items,\n",
    " - k is the number of clusters\n",
    " - d is the number of dimensions of the feature vectors\n",
    " - i is the number of iterations needed until convergence. \n",
    " \n",
    "  For data that has well-defined clusters, $i$ is typically small. In practice, the $k$-means algorithm is very fast. \n",
    "\n",
    "* Lloyds algorithm finds a *local optimum*, not necessarily the *global optimum*\n",
    "\n",
    "  Since the algorithm is fast, it is common to run the algorithm multiple times and pick the solution with the smallest total intra-cluster distance, \n",
    "$$\n",
    "\\sum_{\\ell=1}^k \\sum_{x_i \\in C_\\ell} \\| x_i - \\mu_\\ell\\|^2. \n",
    "$$\n",
    "\n",
    "* The total intra-cluster distance decreases at every iteration of Lloyd's algorithm\n",
    "\n",
    "* The total intra-cluster distance decreases with larger $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-Means with SciKit Learn\n",
    "\n",
    "SciKit learn has a nice [implementaton of the k-means method](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans), which we'll use to cluster various artificial datasets first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import make_blobs, make_moons, load_iris\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')\n",
    "# Create color maps\n",
    "from matplotlib.colors import ListedColormap\n",
    "cmap = ListedColormap([\"#e41a1c\",\"#984ea3\",\"#a65628\",\"#377eb8\",\"#ffff33\",\"#4daf4a\",\"#ff7f00\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A first example\n",
    "\n",
    "First, we'll create data using a function that generates [gaussian blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html#sklearn.datasets.make_blobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# make_blobs generates gaussian blobs, we create 3 blobs\n",
    "n_samples = 500\n",
    "random_state = 170\n",
    "X, y = make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1],  marker=\"o\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now we run the scikit-learn KMeans implementation with function parameters \n",
    "* `n_clusters`: $k=3$ clusters\n",
    "* `n_init`: only one initialization \n",
    "* `init`: random initialization \n",
    "* `max_iter`: only one iteration \n",
    "\n",
    "**Note:** that this is the dumbest possible version of k-means! This is essentially just random initialization + assign closest points.\n",
    "\n",
    "The clusters are plotted using color. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=3, n_init=1, init='random', max_iter=1).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The results are highly dependent on the random initialization of the centroids. \n",
    "\n",
    "If we set the maximum number of iterations to 5, we already see some improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = KMeans(n_clusters=3, n_init=1, init='random', max_iter=5).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### More function arguments\n",
    "\n",
    "* `n_init`: Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. Defaults to 10.\n",
    "\n",
    "* `init`\n",
    " - `random` picks k random points for inital cluster seeds\n",
    " - `k-means++` (the default) uses the [k-means++](https://en.wikipedia.org/wiki/K-means%2B%2B) [algorithm](http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf) to\"spread out\" the inital seeds: the first cluster center is chosen uniformly at random from the data points that are being clustered, after which each subsequent cluster center is chosen from the remaining data points with probability proportional to its squared distance from the point's closest existing cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# For this simple dataset using k-means++ initalization and 10 runs of k-means,\n",
    "# even a max_iter of 1 yields good results.\n",
    "y_pred = KMeans(n_clusters=3, max_iter=1).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## More complicated datasets\n",
    "\n",
    "The total intra-cluster distance, however, makes the assumption that clusters are \n",
    "* convex\n",
    "* isotropic (uniform in all orientations). \n",
    "\n",
    "Up to now, we've applied the algorithm only to nice, Gaussian blobs of equal size. Let's see what the algorithm does for point sets that \n",
    "* vary in size\n",
    "* vary in variance\n",
    "* are anisotropic \n",
    "* are non-convex.\n",
    " \n",
    "### Varying Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "# Unevenly sized blobs\n",
    "X_filtered = np.vstack((X[y == 0][:200], X[y == 1][:50], X[y == 2][:10]))\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X_filtered)\n",
    "plt.scatter(X_filtered[:, 0], X_filtered[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "K-Means does fine for unevenly sized blobs. \n",
    "\n",
    "### Anisotropic point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Anisotropically distributed data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "\n",
    "y_pred = KMeans(n_clusters=3).fit_predict(X_aniso)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred,  marker=\"o\", cmap=cmap); \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Here we see a clear weakness of k-means, it doesn't work well for anisotropic point clouds. \n",
    "\n",
    "### Non-convex point clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_moons, y = make_moons(n_samples=n_samples, noise=.05)\n",
    "\n",
    "y_pred = KMeans(n_clusters=2).fit_predict(X_moons)\n",
    "plt.scatter(X_moons[:, 0], X_moons[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The k-means method also doesn't work well for non-convex point clouds. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choosing k in the k-means method\n",
    "$k$-means is very sensitive to the choice of the parameter, $k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=2).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=n_samples, centers=5, random_state=random_state)\n",
    "y_pred = KMeans(n_clusters=7).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**How do you choose K?** Ideas:\n",
    "\n",
    "1. Visual comparison\n",
    "+ Looking for at which $k$ the total intra-cluster distance tapers off. \n",
    "+ [Silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "The first method is useful when the feature vectors are two dimensional, but it's difficult to visualize data in higher dimensions. Let's consider the other two ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# clustering for k = 1 to k = 10\n",
    "ks = range(1,10)\n",
    "scores = []\n",
    "\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)\n",
    "    model.fit_predict(X)\n",
    "    scores.append(-model.score(X))\n",
    "\n",
    "plt.plot(ks, scores)\n",
    "plt.ylabel('total intra-cluster distance')\n",
    "plt.xlabel('k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We can see that the inertia is large for $k = 1$ and decreases as we increase $k$, until $k=5$, after which it tapers of and gets only marginally smaller. This indicates that $k=5$ is a good choice.\n",
    "\n",
    "Note: the scikit learn implementation returns negative values (\"opposites of the k-means objective\") as scores here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Silhouette Analysis\n",
    "Another choice for choosing the parameter $k$ in $k$-means is [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "\n",
    "Here, we measure the distance between points in one cluster and points in neightboring clusters. \n",
    "of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. \n",
    "\n",
    "\n",
    "\n",
    "+ [`silhouette_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html): The Silhouette Score is the mean Silhouette Coefficient of all samples. The Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is \n",
    "$$ \\frac{b - a}{\\max\\{a, b\\} }.\n",
    "$$\n",
    "This measure has a range of [-1, 1] with  1 = good. \n",
    "\n",
    "There is a nice plot, called a *silhouette plot* which shows the Silhouette Coefficient for each sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.Spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Iris dataset\n",
    "\n",
    "We're going to look at how well k-means does on a higher dimensional example, the well-known Iris dataset. \n",
    "\n",
    "First, let's load the dataset and look at the first two dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y_iris_gt = iris.target\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y_iris_gt,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This isn't very discernible in just two dimensions. We'll see how we can better plot this once we've talked about Priciple Component Analysis (PCA). \n",
    "\n",
    "Now let's run a clustering algorithm using all features (dimensions) and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_iris_pred = KMeans(n_clusters=3,n_init=100).fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_iris_pred,  marker=\"o\", cmap=cmap);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "These clusters looks similar to the ground truth labels.\n",
    "\n",
    "How do we evaluate the qualitiy of the clusters? \n",
    "\n",
    "Just comparing the labels is difficult, since permutations of the labels shouldn't matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(y_iris_pred)\n",
    "print(Y_iris_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluating cluster quality with known ground-truth labels\n",
    "\n",
    "1. [`homogeneity_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html): Homogeneity metric of a cluster labeling given a ground truth. A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.\n",
    "+ [`completeness_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html): A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.\n",
    "+ [`v_measure_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.v_measure_score.html): The V-measure is the harmonic mean between homogeneity and completeness:\n",
    "$$\n",
    "v = 2 * (homogeneity * completeness) / (homogeneity + completeness)\n",
    "$$\n",
    "+ [`homogeneity_completeness_v_measure`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_completeness_v_measure.html): Compute the homogeneity, completeness, and v-Measure scores at once.\n",
    "+ Confusion matrix and purity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "metrics.homogeneity_completeness_v_measure(Y_iris_gt,y_iris_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The homogeneity, completeness, and v-meausre scores are all approximately 76%. This is ok, but not great. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other clustering algorithms and ideas\n",
    "There are many clustering algorithms beyond $k$-means. \n",
    "\n",
    "1. Partitional Algorithms\n",
    " - divide data into set of bins\n",
    " - bins either manually set (e.g., k-means) or automatically determined (e.g., affinity propagation)\n",
    "\n",
    "+ Hierarchical Algorithms\n",
    " - Produce a *dendrogram* or \"similarity tree\", \n",
    " - clusters can be produced by \"cutting\" the dendrogram\n",
    "\n",
    "+ Bi-Clustering\n",
    " - Clusters dimensions & records\n",
    "\n",
    "+ Fuzzy clustering\n",
    " - probabilistic cluster assignment allows occurrence of elements in multiples clusters\n",
    " \n",
    "Many variants of these ideas are implemented in scikit-learn. [Here](http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods)  is an overview. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
