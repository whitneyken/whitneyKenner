{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Neural Networks\n",
    "Adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss Neural Networks, which can be used for both Classification and Regression. In particular, we'll discuss \n",
    "* perceptrons and multi-layer perceptrons (MLP)\n",
    "* neural networks with scikit-learn\n",
    "* how to train a neural network\n",
    "* intro to TensorFlow\n",
    "\n",
    "Recommended Reading:\n",
    "* A. Géron, [Hands-On Machine Learning with Scikit-Learn & TensorFlow](http://proquest.safaribooksonline.com/book/programming/9781491962282) (2017), Ch. 9,10. See also the [associated github page](https://github.com/ageron/handson-ml). \n",
    "* Welch Labs, [Neural Networks demystified](https://github.com/stephencwelch/Neural-Networks-Demystified)\n",
    "* I. Goodfellow and Y. Bengio and A. Courville, [Deep Learning](http://www.deeplearningbook.org/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_iris, make_moons, load_breast_cancer, fetch_openml, fetch_california_housing\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks and deep learning\n",
    "\n",
    "[Artificial Neural Networks](https://en.wikipedia.org/wiki/Artificial_neural_network) were originally motivated by the brain, which is composed of a network of neurons. Each neuron recieves a (chemical) signal from other neurons, does a small computation and then decides if and how to release more chemicals. This composition of small calculations can perform complicated tasks! Similiarly, an artificial neural network is a network composed of neurons, which we simply think of as a computational units. \n",
    "\n",
    "Large scale neural networks are at the core of [deep learning](https://en.wikipedia.org/wiki/Deep_learning), which has gained much publicity for performing very impressive machine learning tasks in the past few years, such as, \n",
    "* classifying billions of images (*e.g.*, Google Images)\n",
    "+ speech recognition (*e.g.*, Amazon's Alexa or Apple’s Siri)\n",
    "+ video recommendation (*e.g.*, YouTube), \n",
    "+ beating the world champion at the game of Go (DeepMind’s AlphaGo).\n",
    "\n",
    "Neural Networks can generally be used for supervised learning tasks, such as classification and regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptrons\n",
    "\n",
    "The simplest element in a neural network is called the [perceptron](https://en.wikipedia.org/wiki/Perceptron). The *perceptron* is an binary classifier. It maps a real input $x \\in \\mathbb R^m$ to a binary output\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases}\n",
    "1 & \\textrm{if } \\ w\\cdot x + b > 0 \\\\\n",
    "0 & \\textrm{otherwise}\n",
    "\\end{cases}.\n",
    "$$\n",
    "Here, $w \\in \\mathbb R^m$ is a vector of weights and $b$ is a scalar called the *bias*. (This is very similiar to the binary classifier we saw when looking at support vector machines.) \n",
    "\n",
    "It is customary to represent this function by the following diagram. \n",
    "\n",
    "<img src=\"Perceptron.svg\" title=\"https://commons.wikimedia.org/wiki/File:Perceptron.svg#/media/File:Perceptron.svg\" width=\"500\">\n",
    "\n",
    "Just like for previous classification methods, we first *train* the network on data, which is to say that we find good choices of $w$ and $b$ for our application. Then we use the neural network to classify new data points. \n",
    "\n",
    "Of course, a single perceptron is only a linear discriminator (similiar to logistic regression and linear SVM). But things become much more interesting when you start composing many neurons, that is, considering networks with more *layers*. \n",
    "\n",
    "The way in which we put together the neurons is referred to as the **network architecture**. There are many ways to do this. Here is a peak at the [neural-network-zoo](http://www.asimovinstitute.org/neural-network-zoo/): \n",
    "<img src=\"neuralnetworks.png\" title=\"http://www.asimovinstitute.org/neural-network-zoo/\" \n",
    "width=\"700\">\n",
    "\n",
    "There are many different network architectures. The most important thing about all of these neural networks is that there is an *input layer*, typically drawn on the left hand side and an *output layer*, typically drawn on the right hand side. The middle layers are sometimes called *hidden layers*. \n",
    "\n",
    "<img src=\"Colored_neural_network.svg\" title=\"https://en.wikipedia.org/wiki/Artificial_neural_network#/media/File:Colored_neural_network.svg\" \n",
    "width=\"500\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In all of these neural network designs, each layer has its own weight vectors and biases that need to be trained. Consequently, *training* a neural network is a much harder job than we have seen for previous methods. It also requires more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks in Practice\n",
    "\n",
    "In the past, we have use scikit-learn for classification. Scikit-learn also has Neural Network library [here](http://scikit-learn.org/stable/modules/neural_networks_supervised.html). However, this implementation does not scale to large-scale applications (no GPU support or deep learning architectures). \n",
    "\n",
    "There are many other packages that have more complete implementations of neural networks. Here is a partial list with short statements taken from the packages.\n",
    "* [Pytorch](https://pytorch.org): Seems to have knocked off Tensoflow as the top choice for most NN researchers/developers.  We'll us it in this course.\n",
    "* [TensorFlow](https://github.com/tensorflow/tensorflow): TensorFlow™ is an open source C++ software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. The flexible architecture allows you to deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device with a single API. \n",
    "+ [Torch](http://torch.ch/): Torch is a scientific computing framework with wide support for machine learning algorithms that puts GPUs first.\n",
    "+ [CNTK](https://github.com/Microsoft/cntk)  Cognitive Toolkit (CNTK) is an open source deep-learning toolkit developed by Microsoft. \n",
    "+ [Theano](https://github.com/Theano/Theano): Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It can use GPUs and perform efficient symbolic differentiation.\n",
    "\n",
    "+ [keras](https://github.com/keras-team/keras): Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\n",
    "+ [MXNet](https://github.com/dmlc/mxnet): Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more\n",
    "+ [Caffe](http://caffe.berkeleyvision.org/): Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by Berkeley AI Research (BAIR) and by community contributors.\n",
    "+ [Lasagne](https://github.com/Lasagne/Lasagne): Lightweight library to build and train neural networks in Theano. \n",
    "+ [prettytensor](https://github.com/google/prettytensor/): \n",
    "Pretty Tensor provides a high level builder API for TensorFlow. It provides thin wrappers on Tensors so that you can easily build multi-layer neural networks.\n",
    "\n",
    "+ [Deeplearning4j](https://deeplearning4j.org/): Open-Source, Distributed, Deep Learning Library for the JVM\n",
    "+ [H2O](https://github.com/h2oai): Fast Scalable Machine Learning For Smarter Applications\n",
    "\n",
    "We'll start with the scikit-learn implementation, since this environment is familiar and then use TensorFlow later. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural networks with scikit-learn\n",
    "\n",
    "scikit-learn has a few different neural network functions:\n",
    "1. [perceptron](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)\n",
    "1. [multi-layer perceptron (MLP) classifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
    "1. [multi-layer perceptron (MLP) regressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
    "\n",
    "The scikit-learn user guide for supervised learning using neural networks is [here](http://scikit-learn.org/stable/modules/neural_networks_supervised.html). \n",
    "\n",
    "Let's first test the `MLPClassifier` on the [two moons dataset](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# X contains two features\n",
    "# y contains labels\n",
    "X,y = make_moons(n_samples=1000,random_state=1,noise=0.2)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "plt.title('Two Moons Dataset')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier( hidden_layer_sizes=(3,3,3), max_iter=1000, alpha=1e-4,\n",
    "                       solver='adam', verbose=10, random_state=1, \n",
    "                       learning_rate_init=.1 )\n",
    "model.fit( X, y ) # What are X and y here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the data, color by class\n",
    "plt.scatter(X[y == 1, 0], X[y == 1, 1], color=\"DarkBlue\", marker=\"s\",label=\"class 1\")\n",
    "plt.scatter(X[y == 0, 0], X[y == 0, 1], color=\"DarkRed\", marker=\"o\",label=\"class 2\")\n",
    "plt.legend(scatterpoints=1)\n",
    "\n",
    "# Plot the predictions made by NN\n",
    "x_min, x_max = X[:,0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),np.linspace(y_min, y_max, 200))\n",
    "zz = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, zz, cmap=ListedColormap(['DarkRed', 'DarkBlue']), alpha=.2)\n",
    "plt.contour(xx, yy, zz, colors=\"black\", alpha=1, linewidths=0.2) \n",
    "\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title('Classification of Two Moons using MLPClassifier')\n",
    "plt.xlabel('feature 1')\n",
    "plt.ylabel('feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('After ', model.n_iter_, ' iterations, the loss is ', model.loss_)\n",
    "print('model coef shapes')\n",
    "[print(coef.shape) for coef in model.coefs_]\n",
    "print('model coefs')\n",
    "[print(coef) for coef in model.coefs_]\n",
    "print('model intercepts')\n",
    "[print(coef) for coef in model.intercepts_]\n",
    "\n",
    "print(model.get_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "There are a lot more function parameters for [`MLPClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) than for other scikit-learn classification methods. You'll find that tweaking them also makes a very big difference in the output. Here are some of the important parameters:\n",
    "\n",
    "#### Network architecture parameters\n",
    "\n",
    "+ **hidden_layer_sizes**: tuple, length = n_layers - 2, default (100,). \n",
    "The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "+ **activation**: {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’. \n",
    "Activation function for the hidden layer.\n",
    " - ‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    " - ‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    " - ‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    " - ‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "#### Optimization related parameters\n",
    "+ **solver**: {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’. \n",
    "The solver for weight optimization.\n",
    " - ‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    " - ‘sgd’ refers to stochastic gradient descent.\n",
    " - ‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "+ **alpha**: float, optional, default 0.0001.\n",
    "L2 penalty (regularization term) parameter.\n",
    "\n",
    "+ **max_iter**: int, optional, default 200. \n",
    "Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "+ **random_state**: int, RandomState instance or None, optional, default None. \n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\n",
    "\n",
    "+ **tol**: float, optional, default 1e-4. \n",
    "Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.\n",
    "\n",
    "+ **verbose**: bool, optional, default False. Whether to print progress messages to stdout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise**: By chainging the hidden_layer_sizes, activation function, and the random_state, see if you can find a better classification of the above two moons dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-layer Perceptrons\n",
    "\n",
    "Here, we take a closer look at Multi-layer Perceptrons; this material is taken from the [scikit-learn user guide](http://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
    "\n",
    "Given a set of features $X = \\{x_1, x_2, ..., x_n\\}$ and a target $y$, a **Multi-layer Perceptron** can learn a non-linear function for either classification or regression. Here is an example with one hidden layer. \n",
    " \n",
    "<img src=\"multilayerperceptron_network.png\" title=\"http://scikit-learn.org/stable/_images/multilayerperceptron_network.png\" \n",
    "width=\"300\">\n",
    "\n",
    "Each neuron applies an affine transformation \n",
    "$$\n",
    "x \\mapsto w\\cdot x + b\n",
    "$$\n",
    "and then a non-linear *activation function* $g\\colon \\mathbb R \\to \\mathbb R$. The composition looks like:\n",
    "$$\n",
    "x \\mapsto g(w\\cdot x + b).\n",
    "$$\n",
    "The output layer receives the values from the last hidden layer and transforms them into output values.\n",
    "\n",
    "In the previous example, we used saw the trained cofficients by printing `model.coefs_` and `model.intercepts_`. \n",
    "\n",
    "There are several choices of **activation function**: hyperbolic tangent, logistic, and rectified linear unit (ReLU). We used the default activiation function, ReLU. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# see Géron, Ch. 10\n",
    "\n",
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def relu(z): # Rectified Linear Unit\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def derivative(f, z, eps=0.000001):\n",
    "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
    "\n",
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.plot(z, np.sign(z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(z, logit(z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, np.tanh(z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Activation functions\", fontsize=14)\n",
    "plt.axis([-5, 5, -1.2, 1.2])\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Step\")\n",
    "plt.plot(0, 0, \"ro\", markersize=5)\n",
    "plt.plot(0, 0, \"rx\", markersize=10)\n",
    "plt.plot(z, derivative(logit, z), \"g--\", linewidth=2, label=\"Logit\")\n",
    "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=2, label=\"Tanh\")\n",
    "plt.plot(z, derivative(relu, z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
    "plt.grid(True)\n",
    "#plt.legend(loc=\"center right\", fontsize=14)\n",
    "plt.title(\"Derivatives\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training a neural network\n",
    "We'll follow the scikit-learn  [user guide](http://scikit-learn.org/stable/modules/neural_networks_supervised.html) \n",
    "to see how the multi-layer perceptron (MLP) neural network is trained. \n",
    "\n",
    "The MLP uses a loss function of the form \n",
    "$$\n",
    "Loss(\\hat{y},y,W) =  \\frac{1}{2} \\sum_{i=1}^n f(\\hat{y}_i(W),y_i) + \\frac{\\alpha}{2} \\|W\\|_2^2\n",
    "$$\n",
    "Here, \n",
    "+ $y_i$ are the labels for the $i$-th example, \n",
    "+ $\\hat{y}_i(W)$ are the predicted label for the $i$-th example, \n",
    "+ $f$ is a function that measures the error, typically $L^2$ difference for regression or cross-entropy for classification, and \n",
    "+ $\\alpha$ is a regularization parameter. \n",
    "\n",
    "Starting from initial random weights, the loss function is minimized by repeatedly updating these weights. The details of this depend on the chosen method, either a quasi-Newton method `lbfgs`, stochastic gradident descent `sgd`, or `adam`. \n",
    "\n",
    "In the **gradient descent method**, the gradient $\\nabla_{W} Loss$ of the loss with respect to the weights is computed. The weights are then changed in the negative gradient direction using a step-length or learning-rate $\\varepsilon>0$: \n",
    "$$\n",
    "W \\leftarrow W - \\varepsilon \\nabla_W {Loss}.\n",
    "$$\n",
    "The algorithm stops when it reaches a preset maximum number of iterations, `max_iter`, \n",
    "or when the improvement in loss is below a preset small number, `tol`.\n",
    "\n",
    "The gradient of $W$ is simply computed using the chain rule from calculus. In principle the idea is simple, but in practice it is a complicated job. Data analysts have figured out a clever way how to organize this calculation. This is sometimes called *back propagation*. \n",
    "\n",
    "A complete description of `lbfgs`, `sgd`, and `adam` is beyond the scope of the course. I'll just say that they are clever modifications to the gradient descent method. \n",
    "\n",
    "Let's see a comparison of optimization methods, taken from [this page](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# different learning rate schedules and momentum parameters\n",
    "params = [{'solver': 'sgd', 'learning_rate': 'constant', 'momentum': 0, # SGD - Stochastic Gradient Descent\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'constant', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': 0,\n",
    "           'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': True, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'sgd', 'learning_rate': 'invscaling', 'momentum': .9,\n",
    "           'nesterovs_momentum': False, 'learning_rate_init': 0.2},\n",
    "          {'solver': 'adam', 'learning_rate_init': 0.01}]\n",
    "\n",
    "labels = [\"constant learning-rate\", \"constant with momentum\",\n",
    "          \"constant with Nesterov's momentum\",\n",
    "          \"inv-scaling learning-rate\", \"inv-scaling with momentum\",\n",
    "          \"inv-scaling with Nesterov's momentum\", \"adam\"]\n",
    "\n",
    "plot_args = [{'c': 'red', 'linestyle': '-'},\n",
    "             {'c': 'green', 'linestyle': '-'},\n",
    "             {'c': 'blue', 'linestyle': '-'},\n",
    "             {'c': 'red', 'linestyle': '--'},\n",
    "             {'c': 'green', 'linestyle': '--'},\n",
    "             {'c': 'blue', 'linestyle': '--'},\n",
    "             {'c': 'black', 'linestyle': '-'}]\n",
    "\n",
    "\n",
    "def plot_on_dataset(X, y, ax, name):\n",
    "    # for each dataset, plot learning for each learning strategy\n",
    "    print(\"\\nlearning on dataset %s\" % name)\n",
    "    ax.set_title(name)\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    mlps = []\n",
    "    if name == \"digits\":\n",
    "        # digits is larger but converges fairly quickly\n",
    "        max_iter = 15\n",
    "    else:\n",
    "        max_iter = 400\n",
    "\n",
    "    for label, param in zip(labels, params):\n",
    "        print(\"training: %s\" % label)\n",
    "        mlp = MLPClassifier(verbose=0, random_state=0,\n",
    "                            max_iter=max_iter, **param)\n",
    "        mlp.fit(X, y)\n",
    "        mlps.append(mlp)\n",
    "        print(\"Training set score: %f\" % mlp.score(X, y))\n",
    "        print(\"Training set loss: %f\" % mlp.loss_)\n",
    "    for mlp, label, args in zip(mlps, labels, plot_args):\n",
    "            ax.plot(mlp.loss_curve_, label=label, **args)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "# load / generate some toy datasets\n",
    "iris = datasets.load_iris()\n",
    "digits = datasets.load_digits()\n",
    "data_sets = [(iris.data, iris.target),\n",
    "             (digits.data, digits.target),\n",
    "             datasets.make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "             datasets.make_moons(noise=0.3, random_state=0)]\n",
    "\n",
    "for ax, data, name in zip(axes.ravel(), data_sets, ['iris', 'digits',\n",
    "                                                    'circles', 'moons']):\n",
    "    plot_on_dataset(*data, ax=ax, name=name)\n",
    "\n",
    "fig.legend(ax.get_lines(), labels, ncol=3, loc=\"upper center\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Some advise on optimization methods according to [this page](http://scikit-learn.org/stable/modules/neural_networks_supervised.html): \n",
    "* Empirically, we observed that L-BFGS converges faster and with better solutions on small datasets. For relatively large datasets, however, Adam is very robust. It usually converges quickly and gives pretty good performance. SGD with momentum or nesterov’s momentum, on the other hand, can perform better than those two algorithms if learning rate is correctly tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "print(cancer.keys())\n",
    "\n",
    "# 569 data points with 30 features\n",
    "cancer['data'].shape\n",
    "\n",
    "# full description:\n",
    "print(cancer['DESCR'])\n",
    "\n",
    "X = cancer['data']\n",
    "y = cancer['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Fit only to the training data\n",
    "\n",
    "# Apply scaling to data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(30,30,30),random_state=1)\n",
    "mlp.fit(X_train,y_train)\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: MNIST dataset\n",
    "\n",
    "Let's train a multi layer perceptron on the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import urllib\n",
    "mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "mnist_path = \"./mnist-original.mat\"\n",
    "response = urllib.request.urlopen(mnist_alternative_url)\n",
    "with open(mnist_path, \"wb\") as f:\n",
    "    content = response.read()\n",
    "    f.write(content)\n",
    "mnist_raw = loadmat(mnist_path)\n",
    "mnist = {\n",
    "    \"data\": mnist_raw[\"data\"].T,\n",
    "    \"target\": mnist_raw[\"label\"][0],\n",
    "    \"COL_NAMES\": [\"label\", \"data\"],\n",
    "    \"DESCR\": \"mldata.org dataset: mnist-original\",\n",
    "}\n",
    "\n",
    "# rescale the data, use the traditional train/test split\n",
    "X, y = mnist['data'] / 255., mnist['target']\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=400, alpha=1e-4,\n",
    "                     solver='adam', verbose=10, tol=1e-4, random_state=1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Training set score: %f\" % mlp.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % mlp.score(X_test, y_test))\n",
    "\n",
    "y_pred = mlp.predict(X_test)\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Exercise**: By adjusting the parameters in the MLPClassifier, imporve the test set score. \n",
    "    \n",
    "\n",
    "**Note**: [This webpage](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mnist_filters.html)\n",
    "tries to interpret the MLP classification weights learned for the MNIST dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: Regression with Neural Networks in scikit-learn\n",
    "\n",
    "Let's use a multi-layer perceptron for regression. This can be done with the scikit-learn [`MLPRegressor`](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "print(housing.keys())\n",
    "\n",
    "# 20640 data points with 8 features\n",
    "housing['data'].shape\n",
    "\n",
    "# full description:\n",
    "print(housing['DESCR'])\n",
    "\n",
    "X = housing['data']\n",
    "y = housing['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2)\n",
    "\n",
    "# scale data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # Fit only to the training data\n",
    "\n",
    "# Apply scaling to data\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Linear Regression with Scikit-Learn\n",
    "lin_reg = LinearRegression()\n",
    "print(lin_reg.get_params())\n",
    "\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(lin_reg.intercept_)\n",
    "print(lin_reg.coef_)\n",
    "\n",
    "print(lin_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# MLP regression with Scikit-Learn\n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(8,8),verbose=0,random_state=2,solver='adam')\n",
    "print(mlp_reg.get_params())\n",
    "\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "print(mlp_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, using the \"identity\" activation function (f(y) = y) even a complex neural network gives the same result as linear regression!  We'll try that below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP regression with Scikit-Learn with identity \n",
    "mlp_reg = MLPRegressor(hidden_layer_sizes=(8,8),verbose=0,random_state=2,solver='adam', activation='identity')\n",
    "print(mlp_reg.get_params())\n",
    "\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "print(mlp_reg.score(X_test,y_test)) # score = 1 is good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Neural Networks demystified\n",
    "\n",
    "There is a good sequence of 7 videos called **Neural Networks demystified** from Welch Labs that builds and trains a neural network *from scratch* in python. \n",
    "* Part 1: [Data + Architecture](https://www.youtube.com/watch?v=bxe2T-V8XRs)\n",
    "* Part 2: [Forward Propagation](https://www.youtube.com/watch?v=UJwK6jAStmg)\n",
    "* Part 3: [Gradient Descent](https://www.youtube.com/watch?v=5u0jaA3qAGk)\n",
    "* Part 4: [Backpropagation](https://www.youtube.com/watch?v=GlcnxUlrtek)\n",
    "* Part 5: [Numerical Gradient Checking](https://www.youtube.com/watch?v=pHMzNW8Agq4&t=22s)\n",
    "* Part 6: [Training](https://www.youtube.com/watch?v=9KM9Td6RVgQ)\n",
    "* Part 7: [Overfitting, Testing, and Regularization](https://www.youtube.com/watch?v=S4ZUwgesjS8)\n",
    "\n",
    "If you're interested in learning more about how Neural Networks are trained, I would recommend watching these videos. \n",
    "\n",
    "The accompanying code is [on github](https://github.com/stephencwelch/Neural-Networks-Demystified) and can be obtained via \n",
    "```\n",
    "git clone https://github.com/stephencwelch/Neural-Networks-Demystified.git\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
