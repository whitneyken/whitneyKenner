{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science – Lecture 12 – Web Scraping\n",
    "*COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/* \n",
    "\n",
    "In this lecture we will explore how we can extract data from a web-page using automatic scraping and crawling with [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n",
    "\n",
    "First, we'll talk a bit about HTML though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML and the DOM\n",
    "\n",
    "We will scrape web-pages that are (partially) written in HTML and represented in the DOM. DOM stands for  Document Object Model, while HTML stands for “HyperText Markup Language”. 25 years ago, that used to be a meaningful description of what HTML actually did: it has links (hypertext), and it is a markup language. The latest version of HTML, however, the HTML5 standard, does much, much more: graphics, audio, video, etc. So it is easier to think of HTML as “whatever it is that web browsers know how to interpret”, and just not think about the actual term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements\n",
    "\n",
    "The important thing about HTML is that the markup is represented by elements. An HTML element is a portion of the content that is surrounded by a pair of tags of the same name. Like this:\n",
    "\n",
    "```html\n",
    "<strong>This is an HTML element.</strong>\n",
    "```\n",
    "\n",
    "In this element, strong is the name of the tag; the open tag is `<strong>`, and the matching closing tag is `</strong>`. The way you should interpret this is that the text “This is an HTML element” should be “strong”, i.e., typically this will be bold text.\n",
    "\n",
    "HTML elements can and commonly do nest:\n",
    "\n",
    "```html\n",
    "<strong>This is strong, and <u>this is underlined and strong.</u></strong>\n",
    "```\n",
    "\n",
    "In addition to the names, opening tags can contain extra information about the element. These are called attributes:\n",
    "\n",
    "```html\n",
    "<a href='http://www.google.com'>A link to Google's main page</a>\n",
    "```\n",
    "\n",
    "In this case, we’re using the `a` element which stood for “anchor”, but now is almost universally used as a “link”. The attribute `href` means “HTML reference”, which actually makes sense for a change. The meaning given to each attribute changes from element to element.\n",
    "\n",
    "Important attributes for our purposes are `id` and `class`. The id attribute gives the attribute a unique identifier, which can then be used to access the element programmatically. Think of it as making the element accessible via a global variable.  \n",
    "\n",
    "The class is conceptually similar but is intendent to be applied to a whole “class” of elements. \n",
    "\n",
    "HTML pages require some boilerplate. Here is a minimal page: \n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title></title>\n",
    "</head>\n",
    "<body>\n",
    "Hello World! What's up?\n",
    "</body>\n",
    "</html>\n",
    "``` \n",
    "\n",
    "The `<head>` contains meta-information such as the titel of the site, the `<body>` contains the actual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchy\n",
    "\n",
    "Data in HTML is often structured hirearchically: \n",
    "\n",
    "```html\n",
    "<body>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 2016-08-25</span>\n",
    "    <span class=\"author\">Led Zeppelin</span>\n",
    "    <h1>Ramble On</h1>\n",
    "    <div class=\"content\">\n",
    "    Leaves are falling all around, It's time I was on my way. \n",
    "    Thanks to you, I'm much obliged for such a pleasant stay. \n",
    "    But now it's time for me to go. The autumn moon lights my way. \n",
    "    For now I smell the rain, and with it pain, and it's headed my way. \n",
    "    <div>\n",
    "  </article>\n",
    "  <article>\n",
    "    <span class=\"date\">Published: 2016-08-23</span>\n",
    "    <span class=\"author\">Radiohead</span>\n",
    "    <h1>Burn the Witch</h1>\n",
    "    <div class=\"content\">\n",
    "    Stay in the shadows\n",
    "    Cheer at the gallows\n",
    "    This is a round up\n",
    "    This is a low flying panic attack\n",
    "    Sing a song on the jukebox that goes\n",
    "    Burn the witch\n",
    "    Burn the witch\n",
    "    We know where you live\n",
    "    <div>\n",
    "  </article>\n",
    "</body>\n",
    "```\n",
    "\n",
    "Here, the title of the song is nested three levels deep: `body > article > h1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables\n",
    "\n",
    "Data is also often stored in HTML tables. `<tr>` indicates a row (table row), `<th>` and `<td>` are used to demark cells, either header cells (`<th>`) or regular cells (`<td>`). \n",
    "\n",
    "```html\n",
    "<table>\n",
    "    <tr>\n",
    "        <th></th>\n",
    "        <th>The Beatles</th>\n",
    "        <th>Led Zeppelin</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td># Band Members</td>\n",
    "        <td>4</td>\n",
    "        <td>4</td>\n",
    "    </tr>\n",
    "</table>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The DOM\n",
    "\n",
    "As we have seen above, a markup document looks a lot like a tree: it has a root, the HTML element, and elements can have children that are containing elements themselves.\n",
    "\n",
    "While HTML is a textual representation of a markup document, the DOM is a programming interface for it. Also the DOM represents the state of a page as it's rendered, that (nowadays) doesn't mean that there is an underlying HTML document that corresponds to that exactly. Rather, the DOM is dynamically generated with, e.g., JavaScript. \n",
    "\n",
    "In this class we will use “DOM” to mean the tree created by the web browsers to represent the document.\n",
    "\n",
    "#### Inspecting the DOM in a browser\n",
    "\n",
    "Perhaps the most important habit when scraping is to investigate the source of a page using the Developer Tools. In this case, we’ll look at the Element tree, by clicking on the menu bar: View → Developer → Developer Tools.\n",
    "\n",
    "Alternatively, you can right click on any part of the webpage, and choose “Inspect Element”. Notice that there can be a big difference between what is in the DOM and what is in the source.\n",
    "\n",
    "Take a look at the DOM of [this html page](lyrics.html). Next, we'll scrape the data from this page! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with BeautifulSoup\n",
    "\n",
    "[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) is a Python library design for computationally extracting data from html documents. It supports navigating in the DOM and retreiving exactly the data elements you need.\n",
    "\n",
    "Let's start with a simple example using the [lyrics.html](lyrics.html) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "<meta charset=\"utf-8\"/>\n",
       "<title>Lyrics</title>\n",
       "</head>\n",
       "<body>\n",
       "<article id=\"zep\">\n",
       "<span class=\"date\">Published: 2016-08-25</span>\n",
       "<span class=\"author\">Led Zeppelin</span>\n",
       "<h1>Ramble On</h1>\n",
       "<div class=\"content\">\n",
       "    Leaves are falling all around, It's time I was on my way.\n",
       "    Thanks to you, I'm much obliged for such a pleasant stay.\n",
       "    But now it's time for me to go. The autumn moon lights my way.\n",
       "    For now I smell the rain, and with it pain, and it's headed my way.\n",
       "    </div>\n",
       "</article>\n",
       "<article id=\"radio\">\n",
       "<span class=\"date\">Published: 2016-08-23</span>\n",
       "<span class=\"author\">Radiohead</span>\n",
       "<h1>Burn the Witch</h1>\n",
       "<div class=\"content\">\n",
       "    Stay in the shadows\n",
       "    Cheer at the gallows\n",
       "    This is a round up\n",
       "    This is a low flying panic attack\n",
       "    Sing a song on the jukebox that goes\n",
       "    Burn the witch\n",
       "    Burn the witch\n",
       "    We know where you live\n",
       "    </div>\n",
       "</article>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#install the bs4 package through pip.  I think the beautifulsoup pacakage is a much older version?\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# we tell BeautifulSoup and tell it which parser to use\n",
    "song_soup = BeautifulSoup( open(\"lyrics.html\"), \"html.parser\" )\n",
    "# the output corresponds exactly to the html file\n",
    "song_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes that can be hard to read, so we can format it\n",
    "print( song_soup.prettify() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access content by tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the title tag\n",
    "song_soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And get the text out of the tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.title.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly accessing an element works for the first occurence of a tag, we don't get the others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can retreive the text content of an element: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( song_soup.div.string )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use attributes to find specific element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup.find( id=\"zep\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can also get only the text, not the html markup: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = song_soup.find( id=\"zep\" ).get_text()\n",
    "print( text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use find_all to get all instances of a tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1s = song_soup.find_all( \"h1\" )\n",
    "h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a list of beautiful soup elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( h1s[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to get the text out of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_h1s = [ tag.get_text() for tag in h1s ]\n",
    "string_h1s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `find_all` is so commonly used, you can use a shortcut by just calling directly on an object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup( \"div\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can address the elements in the returned object directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_soup(\"div\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or iterate over it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in song_soup.find_all(\"div\"):\n",
    "    print(\"---\")\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Selectors\n",
    "\n",
    "We can also use CSS selectors. CSS Selectors apply, among others, to elements, classes, and IDs.\n",
    "\n",
    "Below is an example of how CSS is used to style different elements. \n",
    "\n",
    "\n",
    "```CSS\n",
    "/* Element Selector */\n",
    "article {\n",
    "  color: FireBrick;\n",
    "}\n",
    "\n",
    "/* ID selector */\n",
    "#myID {\n",
    "  color: Tomato;\n",
    "}\n",
    "\n",
    "/* Class selector */\n",
    ".myClass {\n",
    "  color: Aquamarine;\n",
    "}\n",
    "\n",
    "/* Child selector. Only DIRECT children match */\n",
    "p > b {\n",
    "  color: SteelBlue;\n",
    "}\n",
    "\n",
    "/* Descendant selector. Every time a b is nested within a div this matches */\n",
    "div b {\n",
    "  color: green;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "[Here is an example](https://jsfiddle.net/gxhqv26m/1/) with all the important selectors.\n",
    "\n",
    "Let's try this in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all elements of class .content\n",
    "song_soup.select( \".content\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting all divs that are somewhere below the id radio in the tree\n",
    "song_soup.select( \"#radio div\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now we know how to extract information out of a website. Now let's look at a complete example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching a Website\n",
    "\n",
    "Downloading websites is easy and very efficient. It turns out, that you can cause quite high load on a server when you scrape a lot. So webmasters usually publish what kinds of scraping they allow on their websites. You should check out a websites terms of service and the `robots.txt` of a domain before crawling excessively. Terms of service are usually broad, so searching for “scraping” or “crawling” is a good idea.\n",
    "\n",
    "Let's take a look at [Google Scholar's robots.txt](https://scholar.google.com/robots.txt):\n",
    "\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /search\n",
    "Allow: /search/about\n",
    "Disallow: /sdch\n",
    "Disallow: /groups\n",
    "Disallow: /index.html?\n",
    "Disallow: /?\n",
    "Allow: /?hl=\n",
    "...\n",
    "Disallow: /scholar\n",
    "Disallow: /citations?\n",
    "...\n",
    "```\n",
    "\n",
    "Here it specifies that you're not allowed to crawl a lot of the pages. The `/scholar` subdirectory is especially painful because it prohibits you from generating queries dynamically. \n",
    "\n",
    "It's also common that sites ask you to delay crawiling: \n",
    "\n",
    "```\n",
    "Crawl-delay: 30 \n",
    "Request-rate: 1/30 \n",
    "```\n",
    "\n",
    "You should respect those restrictions. Now, no one can stop you from running a request through a crawler, but sites like google scholar will block you VERY quickly if you request to many pages in a short time-frame.\n",
    "\n",
    "An alternative strategy to dynamic crawling (as we're doing in the next example) is to download a local copy of the website and crawl that. This ensures that you hit the site only once per page. A good tool to achieve that is [wget](https://www.gnu.org/software/wget/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Utah's course enrollments\n",
    "\n",
    "We're going to build a dataset of classes offered this fall at the U and look at the enrollment numbers. We'll use the catalog listed here:  \n",
    "https://student.apps.utah.edu/uofu/stu/ClassSchedules/main/1208/\n",
    "\n",
    "The U doesn't seem to care whether/how we crawl the websites, the [fineprint](https://www.utah.edu/disclaimer/) doesn't mentione it and there is no `robots.txt`: http://www.utah.edu/robots.txt\n",
    "\n",
    "We'll use the [`urllib.request`](https://docs.python.org/3.0/library/urllib.request.html) library to retreive the websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = \"https://student.apps.utah.edu/uofu/stu/ClassSchedules/main/1208/\"\n",
    "# here we actually access the website\n",
    "with urllib.request.urlopen( url ) as response:\n",
    "    html = response.read()\n",
    "    html = html.decode( 'utf-8' )\n",
    "\n",
    "# save the file\n",
    "with open( 'class_schedule.html', 'w' ) as new_file:\n",
    "    new_file.write(html)\n",
    "\n",
    "# here it's already a local operation\n",
    "soup = BeautifulSoup( html, 'html.parser' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first 5000 lines of this page: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( soup.prettify()[0:1000] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around with the inspector in chrome you can find the div elements that correspond to a subject description + link:\n",
    "```\n",
    "CS: (Computer Science, https://student.apps.utah.edu/uofu/stu/ClassSchedules/main/1208/class_list.html?subject=CS)\n",
    "```\n",
    "\n",
    "Let's try to grab that info for all subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD (Now Broken) VERSION\n",
    "\n",
    "# # subjects = {}\n",
    "\n",
    "# for subject in soup.find_all( class_=\"subject-list\" ):\n",
    "#     # the url is relative. \n",
    "#     # We can get the tail by retrieving the link out the href attribute of the a tag\n",
    "#     link_tail = subject.find(\"a\").get(\"href\")\n",
    "#     # concatenate the base URL and the tail of the link\n",
    "#     link = url + link_tail\n",
    "#     # the subject shortname is embedded within the <a> tag\n",
    "#     subject_short = subject.find(\"a\").get_text()\n",
    "#     # the subject name is embedded within the span \n",
    "#     subject_long = subject.span.get_text()\n",
    "#     # write it to the dictionary\n",
    "#     subjects[subject_short] = (subject_long, link)\n",
    "\n",
    "# subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oof... The U changed the formatting of this data since the last time I taught this class...  Scraping is VERY FRAGILE.  It looks like NONE of the elements have any sort of useful semantic data in the class/id, so let's try to pull our course links by looking at the href attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = {}\n",
    "for atag in soup.find_all( \"a\" ):\n",
    "    link = atag.get( \"href\" )\n",
    "    if link and link.startswith( \"class_list.html?subject\" ):\n",
    "        subjectShort, subjectLong = atag.get_text().split(' - ', maxsplit=1)\n",
    "        subjects[subjectShort] = (subjectLong, url + link)\n",
    "display( subjects )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects[\"MATH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's what we want. \n",
    "\n",
    "As an aside: we could have taken a different approach here. Note how the URL has a deterministic query parameter that matches the subject:\n",
    "\n",
    "```\n",
    "class_list.html?subject=MATH\n",
    "```\n",
    "\n",
    "We could use this to also retrieve the links if we only had the subject shortnames. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a list of classes\n",
    "\n",
    "Next, it's time to get the courses. Let's look at the [website for CS](https://student.apps.utah.edu/uofu/stu/ClassSchedules/main/1184/class_list.html?subject=CS).\n",
    "\n",
    "We'll fetch this class list in a fucntion where that we pass the subject name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWebsiteAsSoup(url):\n",
    "    \"\"\" \n",
    "    Retrieve a website and return it as a BeautifulSoup object.   \n",
    "    \"\"\"\n",
    "    \n",
    "    #user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.167 Safari/537.36'\n",
    "    #headers = {'User-Agent': user_agent}\n",
    "      \n",
    "    # values = {'name': 'Michael Foord',\n",
    "    #       'location': 'Northampton',\n",
    "    #       'language': 'Python' }\n",
    "    \n",
    "    #data = urllib.parse.urlencode(values)\n",
    "    #data = data.encode('ascii')\n",
    "\n",
    "    #req = urllib.request.Request(url, data, headers)\n",
    "    req = urllib.request.Request( url )\n",
    "    with urllib.request.urlopen( req ) as response:\n",
    "        classlist_html = response.read()\n",
    "    \n",
    "    #print(classlist_html)\n",
    "    \n",
    "    class_soup = BeautifulSoup(classlist_html, 'html.parser')\n",
    "    with open('class_list.html', 'w') as new_file:\n",
    "        new_file.write(str(class_soup))\n",
    "        \n",
    "    return class_soup        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this function for CS and look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_soup = getWebsiteAsSoup( subjects[\"CS\"][1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print( class_soup.select('.class-info')[1].prettify() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find out the number of empty seats for each instructor.\n",
    "\n",
    "Looks like instructors are in a link that starts with http://faculty.utah.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = class_soup.select( '.class-info' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instructors( classes ):\n",
    "    instructors = []\n",
    "    for ci in classes:\n",
    "        instTag = [at.get_text() for at in ci.select(\"a\") \n",
    "                   if at.get(\"href\") and at.get(\"href\").startswith(\"http://faculty.utah.edu\")]\n",
    "        if len( instTag ) > 0: instructors.append( instTag[0] )\n",
    "    return instructors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructors( classes )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of repeats... if we look we'll see that lots of our entries are for labs which we don't care about.  Let's filter those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isLecture(ci):\n",
    "    for st in ci.select(\"span\"):\n",
    "        if st.get_text() == \"Lecture\":\n",
    "            return True\n",
    "    return False\n",
    "lectures = filter(isLecture, classes)\n",
    "lecturingInstructors = instructors(lectures)\n",
    "display(lecturingInstructors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this was super brittle and complicated!  The webpage was clearly NOT designed with us in mind.  Hopefully this convinces you that scraping is a last-resort method of getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Wrap-Up\n",
    "\n",
    "Scraping is a way to get information from website that were not designed to make data accessible. As such, it can often be **brittle**: a website change will break your scraping script. It is also often not welcome, as a scaper can cause a lot of traffic. \n",
    "\n",
    "The way we scraped information here also made the **assumption that HTML is generated consistently** based just on the URL. That is, unfortunately, less and less common, as websites adapt to browser types, resolutions, locales, but also as a lot of content is loaded dynamically e.g., via web-sockets. For example, many websites now auotmatically load more data once you scroll to the bottom of the page. These websites couldn't be scraped with our approach, instead, a browser-emulation approach, using e.g., [Selenium]() would be necessary. [Here is a tutorial](https://medium.com/the-andela-way/introduction-to-web-scraping-using-selenium-7ec377a8cf72) on how to do that. \n",
    "\n",
    "Finally, many services make their data available through a well-defined interface, an API. Using an API is always a better idea than scraping, but scraping is a good fallback!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Exceptional Olympians\n",
    "\n",
    "Scrape data from [this wikipedia site](https://en.wikipedia.org/wiki/List_of_multiple_Olympic_medalists) about exceptional Olympic medalists. \n",
    "\n",
    "1. Download the html using urllib. \n",
    "2. Parse this html with BeautifulSoup.\n",
    "3. Extract the html that corresponds to the big table from the soup.\n",
    "4. Parse the table into a pandas dataframe. Hint: both the \"No.\" and the \"Total.\" column use row-spans which are tricky to parse, both with a pandas reader and manually. For the purpose of this exercise, exclude all rows that are not easy to parse (the first one is Bjørn Dæhlie).\n",
    "5. Create a table that shows for each country how many gold, silver, bronze, and total medals it won in that list.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
