{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "Adapted from *COMP 5360 / MATH 4100, University of Utah, http://datasciencecourse.net/*\n",
    "\n",
    "In this lecture, we'll discuss:\n",
    "* decision trees\n",
    "* generalizability and cross validation \n",
    "\n",
    "Recommended Reading: \n",
    "* G. James, D. Witten, T. Hastie, and R. Tibshirani, An Introduction to Statistical Learning, Ch. 8 [digitial version available here](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "* J. Grus, Data Science from Scratch, Ch.17\n",
    "* [Visual Intro to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "This Lecture is in part based on [this lab](https://github.com/cs109/2015lab7/blob/master/Lab7-Botany%20and%20Ensemble%20Methods.ipynb) and [this blog](http://hamelg.blogspot.com/2015/11/python-for-data-analysis-part-29.html), as well as on the [scikit learn documentation](http://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.style.use('ggplot')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Recall that in *classification* we attempt to predict a categorical variable based on based on several features or attributes. \n",
    "\n",
    "We've already seen two methods for classification: \n",
    "1. Logistic Regression\n",
    "+ k Nearest Neighbors (k-NN)\n",
    "\n",
    "Today, we'll discuss: \n",
    "1. Decision Trees\n",
    "\n",
    "We'll discuss Support Vector Machines tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "We'll consider [a dataset of Titanic passengers](https://www.kaggle.com/c/titanic/) and develop a model to predict whether a particular passenger will survive or not. This is what [the dataset](titanic.csv) looks like:\n",
    "\n",
    "\n",
    "| PassengerId | *Survived* | Pclass | Name                                                                               | Sex    | Age  | SibSp | Parch | Ticket             | Fare     | Cabin           | Embarked |\n",
    "|-------------|----------|--------|------------------------------------------------------------------------------------|--------|------|-------|-------|--------------------|----------|-----------------|----------|\n",
    "| 1           | 0        | 3      | Braund, Mr. Owen Harris                                                            | male   | 22   | 1     | 0     | A/5 21171          | 7.25     |                 | S        |\n",
    "| 2           | 1        | 1      | Cumings, Mrs. John Bradley (Florence Briggs Thayer)                                | female | 38   | 1     | 0     | PC 17599           | 71.2833  | C85             | C        |\n",
    "| 3           | 1        | 3      | Heikkinen, Miss. Laina                                                             | female | 26   | 0     | 0     | STON/O2. 3101282   | 7.925    |                 | S        |\n",
    "| 4           | 1        | 1      | Futrelle, Mrs. Jacques Heath (Lily May Peel)                                       | female | 35   | 1     | 0     | 113803             | 53.1     | C123            | S        |\n",
    "| 5           | 0        | 3      | Allen, Mr. William Henry                                                           | male   | 35   | 0     | 0     | 373450             | 8.05     |                 | S        |\n",
    "\n",
    "\n",
    "Here are the variable descriptions for the non-obvious variables:\n",
    " * **Survived:**        (0 = No; 1 = Yes)\n",
    " * **pclass:**          Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    " * **sibsp:**           Number of Siblings/Spouses Aboard\n",
    " * **parch:**           Number of Parents/Children Aboard\n",
    " * **ticket:**          Ticket Number\n",
    " * **fare:**            Passenger Fare\n",
    " * **cabin:**           Cabin\n",
    " * **embarked:**        Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "\n",
    "\n",
    "We're trying to decide whether a passenger with particular attributes will survive the Titanic disaster, so the **survival** variable is our label, the other columns are the **features**. \n",
    "\n",
    "We will **generate a model based on labeled data (hence \"supervised learning\")**, and ideally this model **should also work for new, similar data**. \n",
    "\n",
    "If a model satisfies this criterion, we say that it is **generalizable**. \n",
    "\n",
    "If a model has 100% accuracy on the training dataset but doesn't generalize to new data, then it isn't a very good model. We say that this model has **overfit** the data. \n",
    "\n",
    "The opposite of overfitting is **underfitting**, which happens when the model isn't complex enough to have good accuracy on the training dataset. \n",
    "\n",
    "**Cross-validation** is a general method for assessing how the results of a model (classification, regression,...) will *generalize* to an independent data set. \n",
    "\n",
    "In classification, cross-validation is a method for assessing how well the classification model will predict the class of points that weren't used to *train* the model. \n",
    "\n",
    "The idea of the method is simple: \n",
    "1. Split the dataset into two groups: the training dataset and the testing dataset. \n",
    "+ Train the model on the training dataset. \n",
    "+ Check the accuracy of the model on the testing dataset. \n",
    "\n",
    "In practice, you have to decide how to split the data into groups (i.e. how large the groups should be). You might also want to repeat the experiment so that the assessment doesn't depend on the way in which you split the data into groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "[Decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning) are very intuitive classification and regression tools. Consider the following tree:\n",
    "\n",
    "![Titanic Decision Tree](titanic_tree.png)\n",
    "\n",
    "This tree predicts the survival of passengers on the titanic.  The figures under the leaves show the probability of survival and the percentage of observations in the leaf.\n",
    "\n",
    "The use of a decision tree is very simple. Suppose someone gives you this tree and a new person. In order to predict whether or not the person would have died on the titanic, you ask the following questions, in order:\n",
    "\n",
    " * Is the person male? If no, we predict they would have survived. If yes, continue.\n",
    " * Is the person older than 9.5 years? If yes, we predict they would have died. If no, continue.\n",
    " * Did the person have more than 3 or more siblings? If yes, we predict they would have died. If no, they would have survived\n",
    "\n",
    "The question we'll move to now is how would one build such a tree? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Decision Tree\n",
    "\n",
    "Building a decision tree isn't really much harder than reading one. Here's the essential rundown of the [ID3 algorithm](https://en.wikipedia.org/wiki/ID3_algorithm):\n",
    "\n",
    " * If the data all have the same label, create a leaf node that predicts that label, and you're done.\n",
    " * If the list of attributes is empty (e.g., because you have used all already), create a leaf node that predicts the most common label.\n",
    " * Else, partition the data by each of the attributes; choose the partition with the lowest error.\n",
    " * Recursively continue on each partitioned subset using the remaining attributes.\n",
    " * Terminate when no attributes are left (see above), or when desired depth or another termination criterion  is reached.\n",
    " \n",
    "So, how do you choose the partition with the lowest error? There are various approaches.\n",
    "\n",
    "Let's say we're building a classification tree by considering a list of predictors. In our example above we want to be able to classify whether people have survived based on things like gender, age, the booked fare, etc. Let's call the variables $X_{i,p}$ ($i$ for passengers, $p$ for predictors). Initially, for the first split, we consider all the passengers and all the predictors. We also have an observed label $Y_i$ for each passenger. \n",
    "\n",
    "We first assign everyone to the same class, say $\\hat{Y}_i = 1$. We can calculate the *sum of squared residuals*, \n",
    "$$\n",
    "SSR = \\sum_i {(\\hat{Y}_i - Y_i)^2}.\n",
    "$$ \n",
    "We want to achieve two things: pick the **best split** for the **best predictor**.\n",
    "\n",
    "- At **each step** of the algorithm we consider a list of possible decisions or splits, *e.g.*, $X_{i,6} > 9$ (age is greater than 9), or $X_{i,5} = female$.\n",
    "- For each possible decision we recalculate the predictor for that rule, for example $\\hat{Y}_i = 1$ if $X_{i,6} > 9$ and $0$ otherwise.\n",
    "- We recalculate the error for each possible decision: $SSR = \\sum_i {(\\hat{Y}_i - Y_i)^2}$\n",
    "- We choose the decision that reduces the error by the largest amount.\n",
    "- Then continue with the next step on the reduced input set.\n",
    "\n",
    "Alternative measures to SSR are the *Gini index* and *cross-entropy*, both measures of the total variance across the classes. \n",
    "\n",
    "In building decision trees, it is easy to overfit the data. There are several methods for avoiding this, which we'll discuss more below. Simple strategies inclue limiting the depth of a tree or only splitting when we have more than $N$ samples left. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a very nice, [interactive tutorial on decision trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) that considers the problem of predicting whether a home is in *San Francisco* or *New York* based on attributes.\n",
    "![](http://www.r2d3.us/static/pages/decision-trees-part-1/preview-en.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees with SciKit Learn\n",
    "\n",
    "Scikit-learn has [a nice decision tree implementation](http://scikit-learn.org/stable/modules/tree.html) which we'll use to learn a tree for our Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv(\"titanic.csv\")\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data cleanup / Imputation\n",
    "\n",
    "We need to do some cleanup: \n",
    " * age has missing values, let's say wherever we have no value, we assume that the person is of mean age (this is not necessarily a good decision)\n",
    " * embarked has missing values, we add a dedicated category for unknown embarkation points\n",
    " * we need to convert the categorical values to numerical values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print( \"Missing ages:\", titanic.Age.isna().sum() )\n",
    "\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna( titanic[\"Age\"].mean() )\n",
    "\n",
    "def sex_to_numeric( x ):\n",
    "    if x == 'male':\n",
    "        return 0\n",
    "    if x == 'female':\n",
    "        return 1\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "titanic[ \"Sex\" ] = titanic[ \"Sex\" ].apply( sex_to_numeric )\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# this will break if run more than once\n",
    "def embarked_to_numeric(x):\n",
    "    if x==\"S\":\n",
    "        return 0\n",
    "    if x==\"C\":\n",
    "        return 1\n",
    "    if x==\"Q\":\n",
    "        return 2\n",
    "    else: \n",
    "        return 3\n",
    "    \n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].apply(embarked_to_numeric)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's work only with the numerical and categorical variables and omit passengerID, Name, Ticket and Cabin. These values could contain some information, but it's hard to make sense of them without more context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "features = [ \"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\" ]\n",
    "titanic = titanic[features]\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploring the data\n",
    "\n",
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.pairplot(titanic, hue=\"Survived\", diag_kind=\"hist\");\n",
    "# Blue == died, Orange == survived\n",
    "# Male == 0, Female == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It's difficult to find clear patterns here, except for the histogram that separates Sex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Our first tree\n",
    "Here is some code that splits the data into training and test sets for cross-validation and selects features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "labels =[ \"Survived\", \"Perished\" ]\n",
    "\n",
    "def splitData( features ):\n",
    "    titanic_predictors = titanic[ features ].values\n",
    "    titanic_labels = titanic[ \"Survived\" ].values\n",
    "\n",
    "    # Split into training and test sets\n",
    "    XTrain, XTest, yTrain, yTest = train_test_split( titanic_predictors, titanic_labels, random_state=1, test_size=0.5 )\n",
    "    return XTrain, XTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And more code for plotting decision trees:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's look at a decision tree that **ONLY operates on sex**! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData( [\"Sex\"] )\n",
    "# fit the tree with the traing data\n",
    "decisionTree = decisionTree.fit( XTrain, yTrain )\n",
    "\n",
    "# predict with the training data\n",
    "y_pred_train = decisionTree.predict( XTrain )\n",
    "# measure accuracy\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "# predict with the test data\n",
    "y_pred = decisionTree.predict( XTest )\n",
    "# measure accuracy\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "\n",
    "_ = tree.plot_tree( decisionTree, filled=True, feature_names=[\"Sex\"] )\n",
    "# renderTree(decisionTree, [\"Sex\"])\n",
    "\n",
    "# Male == 0, Female == 1\n",
    "# 288 Men, 157 Women (In training set)\n",
    "# value => [Died, Lived]\n",
    "# Men: 245 died, 43 lived.\n",
    "# Color: blue   => high likelihood of survival\n",
    "#        orange => high likelihood of death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "~76% correct on the test set isn't bad! - sex seems to be a very good indicator of whether someone has survived or not.\n",
    "\n",
    "Let's add more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Adding more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#  train tree on sex and the number of the number of siblings/spouses aboard\n",
    "used_features = [ \"Sex\", \"SibSp\" ]\n",
    "XTrain, XTest, yTrain, yTest = splitData( used_features )\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit( XTrain, yTrain )\n",
    "\n",
    "y_pred_train = decisionTree.predict( XTrain )\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "_ = tree.plot_tree(decisionTree, filled=True, feature_names=used_features)\n",
    "\n",
    "# Male == Class 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Our accuracy on the training data has gone up, but the accuracy on the test data has gone down. It looks like we're overfitting. But maybe we just selected the wrong features? Let's just try all of them! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_features = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "\n",
    "XTrain, XTest, yTrain, yTest = splitData(all_features)\n",
    "decisionTree = tree.DecisionTreeClassifier()\n",
    "decisionTree = decisionTree.fit(XTrain, yTrain)\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "tree.plot_tree(decisionTree, filled=True, feature_names=all_features); #semicolon means \"don't print the output\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "OK, clearly, we're overfitting the data - 98% accuracy on the training data and only ~75% on the test data. Yet, we've created a complicated tree. \n",
    "\n",
    "Decision trees are notorious for overfitting the data. There are two parameters that help us reign in overfitting:\n",
    "\n",
    "* **max_depth:** The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **min_samples_split:** The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split.\n",
    "\n",
    "Let's try combinations of depth and min split size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize=(15,6) )\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier( max_depth=3 )\n",
    "\n",
    "decisionTree = decisionTree.fit( XTrain, yTrain )\n",
    "\n",
    "y_pred_train = decisionTree.predict( XTrain )\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "\n",
    "_ = tree.plot_tree( decisionTree, filled=True, feature_names=all_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize=(15,6) )\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier( min_samples_split=25 )\n",
    "\n",
    "decisionTree = decisionTree.fit( XTrain, yTrain )\n",
    "\n",
    "y_pred_train = decisionTree.predict(XTrain)\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "\n",
    "tree.plot_tree( decisionTree, filled=True, feature_names= all_features );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure( figsize=(15,6) )\n",
    "\n",
    "decisionTree = tree.DecisionTreeClassifier( max_depth=3, min_samples_split=10 )\n",
    "\n",
    "decisionTree = decisionTree.fit( XTrain, yTrain )\n",
    "\n",
    "y_pred_train = decisionTree.predict( XTrain )\n",
    "print('Accuracy on training data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train)))\n",
    "\n",
    "y_pred = decisionTree.predict(XTest)\n",
    "print('Accuracy on test data = %.1f%%' % (100*metrics.accuracy_score(y_true = yTest, y_pred = y_pred)))\n",
    "tree.plot_tree(decisionTree, filled=True, feature_names=all_features);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "It looks like both, the minimum number of samples for splitting and the maximum depth help with overfitting and we achieve a 79-80% accuracy rate. That doesn't sound much better than just gender alone, but 4% improvement is a lot in classification. Also, in the last combination we have a pretty simple model. The main point seems to be:\n",
    "\n",
    " * Sex is the dominant factor at the root of the tree\n",
    " * For females, if you're in class 1 or 2 you're almost sure to survive\n",
    " * For males, if you were younger than 6.5 years old, you had a chance to survive. \n",
    " * Also note that there are branches that predict the same thing, but with different certainty. For example, in the male / adult category, if you were in \"first class\", you still were likely to die, but less likely than in second or third. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discussion \n",
    "\n",
    "### Advantages of decision trees\n",
    "* Decision trees are simple to explain and interpret\n",
    "* There is a nice graphical display for trees\n",
    "* Easy to handle categorial predictors (no dummy variables needed)\n",
    "\n",
    "### Disadvantages of decision trees\n",
    "* Decision trees generally don't have the predicitve accuracy of other approaches as they tend to overfit the data. \n",
    "* Decision trees are non-robust, *i.e.*, sensitive to small changes in the data.\n",
    "\n",
    "Both of these disadvantages are addressed by the following, more advanced methods which we'll cover next class. \n",
    "\n",
    "### Extensions of decision trees: Bagging, Random Forests, and Boosting\n",
    "\n",
    "* **Bagging.** The idea of [bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating) is to generate several decision trees based on subsets of the data and let the decision trees vote to arrive at a prediciton. Since averaging a set of observations reduces variances (think CLT), this increases the predictive accuracy of the method. \n",
    "\n",
    "* **Random Forests.** Another way to counteract overfitting, called [Random Forests](https://en.wikipedia.org/wiki/Random_forest), is to combine multiple decision trees that were generated with some randomness and let them vote on the result. Here the randomness comes from choosing a random sample of the prediction variables to build each tree. \n",
    "\n",
    "* **Boosting.** [Boosting](https://en.wikipedia.org/wiki/Boosting_(machine_learning) is similiar to bagging, except that the trees are grown sequentially.  \n",
    "\n",
    "These extensions are examples of *ensemble learning*, as they combine multiple learning algorithms. You can read a little about such methods in ISL, Ch. 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrapping\n",
    "from random import randrange\n",
    "\n",
    "def boost( df, N ): # make N data sets from df\n",
    "    ret = []\n",
    "    for i in range( N ):\n",
    "        newDF = df[0:0]#copy the columns/index, but no rows\n",
    "        for row in range( len(df) ):\n",
    "            # rowCopy = df.iloc[randrange(len(df))]\n",
    "            # newDF = newDF.append(rowCopy)\n",
    "\n",
    "            rowCopy = pd.DataFrame( df.iloc[ randrange( len( df ) ) ] )\n",
    "            newDF = pd.concat( [newDF, rowCopy.T], ignore_index=True, axis=0 )\n",
    "        ret.append( newDF )\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanics = boost( titanic, 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = []\n",
    "for i, df in enumerate(titanics):\n",
    "    predictors = df[all_features].values\n",
    "    labels = df[\"Survived\"].values\n",
    "    dt = tree.DecisionTreeClassifier()\n",
    "    dt.fit(predictors, labels)\n",
    "    dts.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "total = np.zeros((len(titanic),))\n",
    "for i, dt in enumerate(dts):\n",
    "    y_pred = dt.predict(titanic[all_features].values)\n",
    "    total += y_pred\n",
    "voteResult = np.zeros(total.shape)\n",
    "for i in range(len(titanic)):\n",
    "    if total[i] >= len(dts)/2:\n",
    "        voteResult[i] = 1\n",
    "    else:\n",
    "        voteResult[i] = 0\n",
    "#Note, I'm not doing cross validation, so take this with a big grain of salt!!!\n",
    "print('Accuracy = ', metrics.accuracy_score(y_true = titanic[\"Survived\"].values, y_pred = voteResult))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree( dts[0], filled=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = tree.plot_tree( dts[-1], filled=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier() #defaults to 100 trees, no limit on depth/leaf size\n",
    "clf = clf.fit(XTrain, yTrain)\n",
    "y_pred_train = clf.predict(XTrain)\n",
    "print('Accuracy on training data= ', metrics.accuracy_score(y_true = yTrain, y_pred = y_pred_train))\n",
    "\n",
    "y_pred = clf.predict(XTest)\n",
    "print('Accuracy on test data= ', metrics.accuracy_score(y_true = yTest, y_pred = y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even for deep/tall trees, we overfit less than we did with a single tree"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "interpreter": {
   "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
