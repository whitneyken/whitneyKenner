{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoders\n",
    "\n",
    "Here we'll apply an autoencoder to the MNIST dataset to see how much we can compress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as we did for CNNs\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5),(0.5))]) #convert from images to tensors\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(\"./mnist\", train=True, download=True, transform=transform)\n",
    "mnist_test = torchvision.datasets.MNIST(\"./mnist\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with \"dumb\" networks that don't use convolution layers, and treat the image as a 1D vector with 28*28 entries.  Our first network will have 2 dense layers for the encoder, and 2 symmteric layers for the decoder, with a compressed representation of size 64 (over 10x reduction from the original size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1\n",
    "\n",
    "class Net( nn.Module ):\n",
    "\n",
    "    def __init__( self, inputLen ):\n",
    "        super(Net, self).__init__()\n",
    "        self.inputLen = inputLen\n",
    "        self.encodedSize = 64\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.inputLen, 128)\n",
    "        self.fc2 = nn.Linear(128, self.encodedSize) #scale down to 64 features\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.encodedSize, 128)\n",
    "        self.fc4 = nn.Linear(128, self.inputLen)\n",
    "        \n",
    "    def compress( self, x ):\n",
    "        x = F.relu( self.fc1(x) )\n",
    "        x = F.relu( self.fc2(x) )\n",
    "        # Now we have a 64-d representation of our data.  If we were doing compression, we'd store this.\n",
    "        return x\n",
    "    \n",
    "    def decompress( self, x ):\n",
    "        x = F.relu( self.fc3(x) )\n",
    "        x = self.fc4( x )\n",
    "        return x\n",
    "    \n",
    "    def forward( self, x ):\n",
    "        x = self.compress( x )\n",
    "        x = self.decompress( x )\n",
    "        return x\n",
    "\n",
    "net = Net( 28*28 ) # treat these as just 28 D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean-squared error loss\n",
    "# this is the normal \"least squares\" error function, divided by the number of data points\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train( model, epochs ):\n",
    "    # create an optimizer object\n",
    "    # Adam optimizer with learning rate 1e-3\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=8, shuffle=True, num_workers=0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            #same as yesterday, except we're not even looking at the labels!\n",
    "            # since we're not using a CNN, we need to \"flatten\" the input images\n",
    "            batch_features = data[0].view(-1, 784)\n",
    "        \n",
    "            # reset the gradients back to zero\n",
    "            # PyTorch accumulates gradients on subsequent backward passes\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute reconstructions\n",
    "            outputs = model(batch_features)\n",
    "        \n",
    "            # compute training reconstruction loss\n",
    "            # again, same idea as yesterday, but we're measuring the error slightly differently\n",
    "            # how well does the reconstructed image match the input image?\n",
    "            train_loss = criterion( outputs, batch_features )\n",
    "        \n",
    "            # compute accumulated gradients\n",
    "            train_loss.backward()\n",
    "        \n",
    "            # perform parameter update based on current gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            loss += train_loss.item()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += train_loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.8f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    \n",
    "        # compute the epoch training loss\n",
    "        loss = loss / len(train_loader)\n",
    "    \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}\".format(epoch + 1, epochs, loss))\n",
    "\n",
    "def evaluate(model):\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=8, shuffle=True, num_workers=0)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images = data[0].view(-1, 784)\n",
    "            outputs = model(images)\n",
    "            test_loss = criterion(outputs, images)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "    print(\"overall loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "\n",
    "print( \"Train: (%d epochs)\" % epochs ) \n",
    "train( net, epochs )\n",
    "\n",
    "print( \"Evaluate:\" )\n",
    "evaluate( net )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does that number mean?  It's probably easiest to get a sense for how good the network is by looking at the reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawComparisons( model ):\n",
    "\n",
    "    plt.figure( figsize=(20, 15) )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader( mnist_test, batch_size=8, shuffle=True, num_workers=0 )\n",
    "    \n",
    "    for i, batch in enumerate( test_loader ):\n",
    "        if i >= 4:\n",
    "            break\n",
    "        images = batch[0]\n",
    "        #print(images.shape)\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(images.view(-1, 28*28))\n",
    "            for j in range( len(images) ):\n",
    "                # Display the original image\n",
    "                ax = plt.subplot(16, 8, i*16 + j + 1)\n",
    "                plt.imshow(images[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "                # and the reconstructed version in the next row\n",
    "                ax = plt.subplot(16, 8, i*16 + j + 1 + 8)\n",
    "                plt.imshow(reconstructed[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drawComparisons( net )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like this network works really well except for some reseidual \"grey\" noise!  We could try feeding these into our classifier from yesterday as well to see if our old network can still identify the digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to go deeper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on https://medium.com/pytorch/implementing-an-autoencoder-in-pytorch-19baa22647d1\n",
    "\n",
    "class DeeperNet(nn.Module):\n",
    "    def __init__(self, inputLen):\n",
    "        super(DeeperNet, self).__init__()\n",
    "        self.inputLen = inputLen\n",
    "        self.encodedSize = 32\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.inputLen, 128)\n",
    "        self.fc2 = nn.Linear(128, 64) #scale down to 64 features\n",
    "        self.fc3 = nn.Linear(64, self.encodedSize) #scale down to 64 features\n",
    "        \n",
    "        \n",
    "        self.fc4 = nn.Linear(self.encodedSize, 64)\n",
    "        self.fc5 = nn.Linear(64, 128)\n",
    "        self.fc6 = nn.Linear(128, self.inputLen)\n",
    "        \n",
    "    def compress(self, x):    \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #now we have a low-d representation of our data.  If we were doing compression, we'd store this\n",
    "        return x\n",
    "    \n",
    "    def decompress(self, x):\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "deeperNet = DeeperNet(28*28) # treat these as just 28 D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train( deeperNet, 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate( deeperNet )\n",
    "drawComparisons( deeperNet )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced representation is half the size as before, so not too surprising the error is higher, but this still looks pretty good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the reduced representation size to see how small we can go while getting reasonable results\n",
    "class DeeperNarrowerNet(nn.Module):\n",
    "    def __init__(self, inputLen, encodedSize):\n",
    "        super(DeeperNarrowerNet, self).__init__()\n",
    "        self.inputLen = inputLen\n",
    "        self.encodedSize = encodedSize\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.inputLen, 128)\n",
    "        self.fc2 = nn.Linear(128, 64) #scale down to 64 features\n",
    "        self.fc3 = nn.Linear(64, self.encodedSize) #scale down to 64 features\n",
    "        \n",
    "        \n",
    "        self.fc4 = nn.Linear(self.encodedSize, 64)\n",
    "        self.fc5 = nn.Linear(64, 128)\n",
    "        self.fc6 = nn.Linear(128, self.inputLen)\n",
    "        \n",
    "    def compress(self, x):    \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        #now we have a low-d representation of our data.  If we were doing compression, we'd store this\n",
    "        return x\n",
    "    \n",
    "    def decompress(self, x):\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "       \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeperNarrowerNet8 = DeeperNarrowerNet(28*28, 8) # treat these as just 28 D vectors\n",
    "train(deeperNarrowerNet8, 4)\n",
    "evaluate(deeperNarrowerNet8)\n",
    "drawComparisons(deeperNarrowerNet8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeperNarrowerNet16 = DeeperNarrowerNet(28*28, 16) # treat these as just 28 D vectors\n",
    "train(deeperNarrowerNet16, 4)\n",
    "evaluate(deeperNarrowerNet16)\n",
    "drawComparisons(deeperNarrowerNet16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll see if we can use this for anomaly detection.  I have a couple of \"hand written\" digits and some letters.  Hopefully the reconstruction error for the letters will be higher than for the digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imgs = [transforms.Normalize((0.5),(0.5))( #scale things the same way we did for our training/testing data\n",
    "    transforms.ToTensor()( #convert to a pytorch tensor\n",
    "        np.array(ImageOps.invert( #in my images, black/white are inverted, so switch them back\n",
    "            Image.open(fname).convert('L') # load the image, and convert to greyscale\n",
    "        )))) for fname in \n",
    "        [\"j.png\", \"k.png\", \"4.png\"]]\n",
    "#duplicate them so it's the length of a batch\n",
    "imgs = torch.stack(imgs)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = deeperNarrowerNet8(imgs.view(-1, 28*28))\n",
    "    individualLosses = nn.MSELoss()\n",
    "    \n",
    "    \n",
    "    for j in range(3):\n",
    "        losses = individualLosses(outputs[j].reshape(1,1, 28,28), imgs[j])\n",
    "        print(losses)\n",
    "        ax = plt.subplot(2, 3, j + 1)\n",
    "        plt.imshow(outputs[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        ax = plt.subplot(2, 3, j + 1 + 3)\n",
    "        plt.imshow(imgs[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As hoped/expected, the \"K\" has the highest reconstruction error... it looks the least like any of the digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using CNN layers, since we're working with images after all..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnNet, self).__init__()\n",
    "        \n",
    "        self.encodedSize = 32\n",
    "        \n",
    "        self.c1Out = 8 # filters from first conv layer\n",
    "        self.c2Out = 8 # filters from 2nd conv layer\n",
    "        \n",
    "        #the padding here puts a \"border\" of 0s around the image, so that convolution layers don't \"shrink\" the image\n",
    "        \n",
    "        self.cv1 = nn.Conv2d(1, self.c1Out, 3, padding=1) #stick with 3x3 filters\n",
    "        #output is 8x 28x28 images\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.cv2 = nn.Conv2d(self.c1Out, self.c2Out, 3, padding=1)\n",
    "        #reuse pool here\n",
    "        \n",
    "        self.downscaledSize = 28//4 #we add padding, so the conv2d layers don't change the size, just the max pools\n",
    "        self.flattenedSize = self.downscaledSize*self.downscaledSize*self.c2Out\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattenedSize, 64)\n",
    "        self.fc2 = nn.Linear(64, self.encodedSize) #scale down to 64 features\n",
    "\n",
    "        #now we're encoded, so go define decoding pieces\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.encodedSize, 64) #scale down to 64 features\n",
    "        self.fc4 = nn.Linear(64, self.flattenedSize)\n",
    "        \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        # the padding is very important here so we don't have to guess a \"frame\" of pixels around the image\n",
    "        self.cv3 = nn.Conv2d(self.c2Out, self.c1Out, 3, padding=1)\n",
    "        # apply upsample again\n",
    "        self.cv4 = nn.Conv2d(self.c1Out, 1, 3, padding=1)\n",
    "        \n",
    "        \n",
    "    def compress(self, x):\n",
    "        x = self.cv1(x)\n",
    "        #print(\"shape after cv1\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"shape after pool1\", x.shape)\n",
    "        x = self.cv2(x)\n",
    "        #print(\"after cv2\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"after pool 2\", x.shape)\n",
    "        x = x.view(-1, self.flattenedSize)\n",
    "        #print(\"flattened shape\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #now we have a low-d representation of our data.  If we were doing compression, we'd store this\n",
    "        return x\n",
    "    \n",
    "    def decompress(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, self.c2Out, self.downscaledSize, self.downscaledSize)\n",
    "        #print(\"unflattened shape\", x.shape)\n",
    "        x = self.upsample(x)\n",
    "        #print(\"upsample\", x.shape)\n",
    "        x = F.relu(self.cv3(x))\n",
    "        #print(x.shape, \"after cv3\")\n",
    "        x = self.cv4(self.upsample(x))\n",
    "        #print(x.shape, \"after both upsamples\")\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "cnnNet = CnnNet() # treat these as just 28 D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#same as we did before except for the reshaping.  CNNs understand 2D images, so we don't want to flatten\n",
    "\n",
    "# mean-squared error loss\n",
    "# this is the normal \"least squares\" error function, divided by the number of data points\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def trainCNN(model, epochs):\n",
    "    # create an optimizer object\n",
    "    # Adam optimizer with learning rate 1e-3\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=8, shuffle=True, num_workers=0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss = 0\n",
    "        \n",
    "        running_loss = 0\n",
    "        \n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "            #same as yesterday, except we're not even looking at the labels!\n",
    "            # since we're not using a CNN, we need to \"flatten\" the input images\n",
    "            batch_features = data[0]\n",
    "        \n",
    "            # reset the gradients back to zero\n",
    "            # PyTorch accumulates gradients on subsequent backward passes\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            # compute reconstructions\n",
    "            outputs = model(batch_features)\n",
    "            #print(batch_features.shape)\n",
    "            #print(outputs.shape)\n",
    "            # compute training reconstruction loss\n",
    "            # again, same idea as yesterday, but we're measuring the error slightly differently\n",
    "            # how well does the reconstructed image match the input image?\n",
    "            train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "            # compute accumulated gradients\n",
    "            train_loss.backward()\n",
    "        \n",
    "            # perform parameter update based on current gradients\n",
    "            optimizer.step()\n",
    "        \n",
    "            # add the mini-batch training loss to epoch loss\n",
    "            loss += train_loss.item()\n",
    "    \n",
    "            # print statistics\n",
    "            running_loss += train_loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] loss: %.8f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    \n",
    "        # compute the epoch training loss\n",
    "        loss = loss / len(train_loader)\n",
    "    \n",
    "        # display the epoch training loss\n",
    "        print(\"epoch : {}/{}, loss = {:.8f}\".format(epoch + 1, epochs, loss))\n",
    "\n",
    "def evaluateCNN(model):\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=8, shuffle=True, num_workers=0)\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images = data[0]\n",
    "            outputs = model(images)\n",
    "            test_loss = criterion(outputs, images)\n",
    "            total_loss += test_loss.item()\n",
    "\n",
    "    print(\"overall loss: \", total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCNN(cnnNet, 4)\n",
    "evaluateCNN(cnnNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawComparisonsCNN(model):\n",
    "    test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=8, shuffle=True, num_workers=0)\n",
    "    plt.figure(figsize=(20, 25))\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        if i >= 8: break\n",
    "        images = batch[0]\n",
    "        #print(images.shape)\n",
    "        with torch.no_grad():\n",
    "            reconstructed = model(images)\n",
    "            for j in range(len(images)):\n",
    "                #draw the original image\n",
    "                ax = plt.subplot(16, 8, i*16 + j + 1)\n",
    "                plt.imshow(images[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "            \n",
    "                #and the reconstructed version in the next row\n",
    "                ax = plt.subplot(16, 8, i*16 + j + 1 + 8)\n",
    "                plt.imshow(reconstructed[j].reshape((28,28)), cmap=\"Greys\", interpolation=None)\n",
    "                ax.get_xaxis().set_visible(False)\n",
    "                ax.get_yaxis().set_visible(False)\n",
    "        \n",
    "drawComparisonsCNN(cnnNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that looks super good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnTinyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnTinyNet, self).__init__()\n",
    "        \n",
    "        self.encodedSize = 4\n",
    "        \n",
    "        self.c1Out = 8 # filters from first conv layer\n",
    "        self.c2Out = 8 # filters from 2nd conv layer\n",
    "        \n",
    "        #the padding here puts a \"border\" of 0s around the image, so that convolution layers don't \"shrink\" the image\n",
    "        \n",
    "        self.cv1 = nn.Conv2d(1, self.c1Out, 3, padding=1) #stick with 3x3 filters\n",
    "        #output is 8x 28x28 images\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.cv2 = nn.Conv2d(self.c1Out, self.c2Out, 3, padding=1)\n",
    "        #reuse pool here\n",
    "        \n",
    "        self.downscaledSize = 28//4 #we add padding, so the conv2d layers don't change the size, just the max pools\n",
    "        self.flattenedSize = self.downscaledSize*self.downscaledSize*self.c2Out\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.flattenedSize, 64)\n",
    "        self.fc2 = nn.Linear(64, self.encodedSize) #scale down to 64 features\n",
    "\n",
    "        #now we're encoded, so go define decoding pieces\n",
    "        \n",
    "        self.fc3 = nn.Linear(self.encodedSize, 64) #scale down to 64 features\n",
    "        self.fc4 = nn.Linear(64, self.flattenedSize)\n",
    "        \n",
    "        \n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear')\n",
    "        # the padding is very important here so we don't have to guess a \"frame\" of pixels around the image\n",
    "        self.cv3 = nn.Conv2d(self.c2Out, self.c1Out, 3, padding=1)\n",
    "        # apply upsample again\n",
    "        self.cv4 = nn.Conv2d(self.c1Out, 1, 3, padding=1)\n",
    "        \n",
    "        \n",
    "    def compress(self, x):\n",
    "        x = self.cv1(x)\n",
    "        #print(\"shape after cv1\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"shape after pool1\", x.shape)\n",
    "        x = self.cv2(x)\n",
    "        #print(\"after cv2\", x.shape)\n",
    "        x = F.relu(self.pool(x))\n",
    "        #print(\"after pool 2\", x.shape)\n",
    "        x = x.view(-1, self.flattenedSize)\n",
    "        #print(\"flattened shape\", x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #now we have a low-d representation of our data.  If we were doing compression, we'd store this\n",
    "        return x\n",
    "    \n",
    "    def decompress(self, x):\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        #print(x.shape)\n",
    "        x = x.view(-1, self.c2Out, self.downscaledSize, self.downscaledSize)\n",
    "        #print(\"unflattened shape\", x.shape)\n",
    "        x = self.upsample(x)\n",
    "        #print(\"upsample\", x.shape)\n",
    "        x = F.relu(self.cv3(x))\n",
    "        #print(x.shape, \"after cv3\")\n",
    "        x = self.cv4(self.upsample(x))\n",
    "        #print(x.shape, \"after both upsamples\")\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.compress(x)\n",
    "        x = self.decompress(x)\n",
    "       \n",
    "        return x\n",
    "\n",
    "cnnTinyNet = CnnTinyNet() # treat these as just 28 D vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCNN(cnnTinyNet, 4)\n",
    "evaluateCNN(cnnTinyNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawComparisonsCNN(cnnTinyNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed = np.array([.2,.2,.2,.2])\n",
    "compressed = torch.tensor(compressed, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = cnnTinyNet.decompress(compressed)\n",
    "plt.imshow(output.detach().numpy().reshape(28,28), cmap=\"Greys\", interpolation=None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fcd46d531329f49b73a0948faa8aed429eb8dafd35203d9948e8358a307ba0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
