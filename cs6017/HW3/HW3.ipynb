{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whitney Kenner\n",
    "HW3\n",
    "6/12/23\n",
    "u0777962\n",
    "CS 6017"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: Data Acquisition\n",
    "\n",
    "note: accidentally did part 1 using API calls before realizing I was supposed to scrape, so the following 2 sections are commented out because it was my original incorrect work RIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Grab the first 5 pages of stories from hackernews.\n",
    "# import requests\n",
    "# import json\n",
    "# response = requests.get(\"https://hacker-news.firebaseio.com/v0/topstories.json?print=pretty\")\n",
    "# print(\"Status code: \", response.status_code)\n",
    "# submissionIDs = response.json()\n",
    "# print(submissionIDs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# import pandas as pd\n",
    "\n",
    "# submissions_dict = []\n",
    "# for subID in submissionIDs[:150]:\n",
    "#     temp_dict = {}\n",
    "#     url = \" https://hacker-news.firebaseio.com/v0/item/\" + str(subID) + \".json?print=pretty\"\n",
    "#     article = requests.get(url)\n",
    "#     #print(f\"id: {subID} \\t status: {article.status_code}\")\n",
    "#     article_j = article.content.decode( \"utf-8\" )\n",
    "#     article_dict = json.loads(article_j)\n",
    "\n",
    "#     temp_dict[\"rank\"] = article_dict['id']\n",
    "#     temp_dict[\"length title\"] = len(article_dict['title'])\n",
    "#     temp_dict[\"time\"] = datetime.datetime.fromtimestamp(article_dict['time']).strftime(\"%H\")\n",
    "#     if 'score' in article_dict:\n",
    "#         temp_dict[\"points\"] = article_dict['score']\n",
    "#     else:\n",
    "#         temp_dict[\"points\"] = 0\n",
    "#     if 'kids' in article_dict:\n",
    "#         temp_dict[\"num comments\"] = len(article_dict['kids']) \n",
    "#     else:\n",
    "#         temp_dict[\"num comments\"] = 0\n",
    "\n",
    "#     submissions_dict.append(temp_dict)\n",
    "\n",
    "# #print(json.dumps(submissions_dict, indent=4))\n",
    "\n",
    "# #convert to dataframe\n",
    "# submissions_df = pd.DataFrame(submissions_dict)\n",
    "\n",
    "# #save as csv file\n",
    "# submissions_df.to_csv('hackerPosts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct work begins\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import string \n",
    "import re\n",
    "import time\n",
    "\n",
    "ranks = []\n",
    "title_lengths = []\n",
    "age_in_hours = []\n",
    "points = []\n",
    "comment_counts = []\n",
    "\n",
    "for i in range(5):\n",
    "    url= \"https://news.ycombinator.com/?p=\" + str(i+1)\n",
    "\n",
    "    with urllib.request.urlopen (url) as response:\n",
    "        html = response.read()\n",
    "        html = html.decode(\"utf-8\")\n",
    "\n",
    "    with open (\"hackernews\" + str(i+1) + \".html\", \"w\") as new_file:\n",
    "        new_file.write(html)\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    #scraping rank\n",
    "    for post in soup.find_all(class_=\"rank\"):\n",
    "        rank = str(post.text)\n",
    "        rank = rank.replace('.','')\n",
    "        ranks.append(int(rank))\n",
    "    #print(posts)\n",
    "\n",
    "    #scraping length of title\n",
    "    for title in soup.find_all(class_=\"titleline\"):\n",
    "        title_lengths.append(len(title.text))\n",
    "\n",
    "    #scraping age of post\n",
    "    for age  in soup.find_all(class_=\"age\"):\n",
    "        age_str = str(age.text)\n",
    "        #print(age.text)\n",
    "        age_str = age_str.removesuffix(\" hours ago\")\n",
    "        age_str = age_str.removesuffix(\" hour ago\")\n",
    "        if \" day ago\" in age_str:\n",
    "            age_in_hours.append(24)\n",
    "        elif \" days ago\" in age_str:\n",
    "            modified_string = age_str.replace(\" days ago\", \"\")\n",
    "            age_int = int(modified_string)\n",
    "            age_in_hours.append(24*age_int)\n",
    "        elif \" minutes ago\" in age_str:\n",
    "            age_in_hours.append(0)\n",
    "        elif \" minute ago\" in age_str:\n",
    "            age_in_hours.append(0)\n",
    "        else:\n",
    "            age_in_hours.append(int(age_str))\n",
    "\n",
    "    #scraping points and comments\n",
    "    for subtext in soup.find_all(class_=\"subtext\" ):\n",
    "        point = 0\n",
    "        comments = 0\n",
    "        for score in subtext.find_all(class_=\"score\"):\n",
    "            point = int(re.search(r'\\d+', str(score.text)).group())\n",
    "\n",
    "        for atag in subtext.find_all(\"a\"):\n",
    "            if atag.text.endswith(\"comments\"):\n",
    "                a_string = str(atag.text)\n",
    "                comments = int(re.search(r'\\d+', a_string).group())\n",
    "        comment_counts.append(int(comments))\n",
    "        points.append(int(point))\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "#creating a data frame\n",
    "submissions_df1 = pd.DataFrame({\"rank\": ranks, \"titleLength\": title_lengths, \"ageInHours\": age_in_hours, \"points\": points, \"commentCount\": comment_counts})\n",
    "print(submissions_df1)\n",
    "\n",
    "# #save as csv file\n",
    "submissions_df1.to_csv('hackerPosts.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 2: Regression\n",
    "\n",
    "We're interested in how to get a high-ranking story on Hackernews. Explore several possible least squares regressions to predict a story's rank based on the other variables (and combinations thereof). Include at least 3 different regressions. Compare/contrast them. Which is the most useful.  What are the R^2 scores, p-values for coefficients, and values of the coefficients - and what do these tell us?  Plot at least one of your regressions (see my example below.)\n",
    "\n",
    "Are there linear relationships between any of the variables? \n",
    "\n",
    "Note: There are a number of issues that will cause our models to not match the real ranking of articles (to a high percentage).\n",
    "\n",
    "There is a complicated formula being used to rank the articles that doesn't directly map to our data.\n",
    "The displayed rank of an article is not immediately/continuously updated.\n",
    "The age (in hours) we are recording for any article over a day old is approximate.\n",
    "Because of this, do not expect to see an R^2 value of over 40% for any of the predictive formulas that you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as sm\n",
    "\n",
    "submissions_df = pd.read_csv('hackerPosts.csv')\n",
    "age_mult_ols = sm.ols(formula=\"rank ~ commentCount + points + ageInHours\", data=submissions_df).fit()\n",
    "age_mult_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment count vs points\n",
    "#this is a bad comparison because neither are independant/dependant\n",
    "comment_points_ols = sm.ols(formula=\"commentCount ~ points\", data=submissions_df).fit()\n",
    "comment_points_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age vs comment count \n",
    "comment_age_ols = sm.ols(formula=\"commentCount ~ ageInHours\", data=submissions_df).fit()\n",
    "comment_age_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age and rank\n",
    "age_rank_ols = sm.ols(formula=\"rank ~ ageInHours\", data=submissions_df).fit()\n",
    "age_rank_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "submissions_df.plot.scatter(x='ageInHours', y='rank')\n",
    "plt.plot(submissions_df[\"ageInHours\"], age_rank_ols.predict(submissions_df[\"ageInHours\"]), linewidth=3, color=\"black\")\n",
    "plt.xlabel(\"Age in Hours\")\n",
    "plt.ylabel(\"Rank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank and points + comments\n",
    "rank_points_ols = sm.ols(formula=\"rank ~ commentCount + points\", data=submissions_df).fit()\n",
    "rank_points_ols.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank vs points + age\n",
    "\n",
    "#best one so far!!!!\n",
    "#rank vs points + age\n",
    "rank_points_ols = sm.ols(formula=\"rank ~ points + ageInHours\", data=submissions_df).fit()\n",
    "rank_points_ols.summary()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is the most useful.  What are the R^2 scores, p-values for coefficients, and values of the coefficients - and what do these tell us?\n",
    "\n",
    "- The most useful least squares regression I found (for rank) was rank vs points and age in hours\n",
    "- The closer the R^2 is to 1, the stronger the correlation, so this R^2 being 0.421 meaning this model explains 42.1% of the variability in rank. soooooooo not great but the highest R^2 I could find (with good P values). \n",
    "- The P values are 0.00 for both points and age in hours, so this is considered to be significant!! This P value indicates that the data is very compatible with this statistcial model \n",
    "- The values of the coefficients are: -0.0711 for points and 3.1954 for ageInHours. This is the amount that the dependant variable (rank) will shift based on a 1 unit shift in each of these independant variables. this is very interesting because it indicated that the points go up, the rank marginally goes down...... huh\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there linear relationships between any of the variables? \n",
    "comments vs points had the strongest linear relationship, but it still onlu had an R squared value of 0.529, so it is not a STONG linear relationship, but it still exists "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3 - Classification\n",
    "\n",
    "As smart people, we know that your rank on HN doesn't matter, as long as you're on the front page. Use logistic regression to attempt to classify whether or not an article will be on the front page, given the other (non-rank) variables. Note, you'll need to transform the rank variable into an indicator variable (1 for front page, 0 for not), for example.\n",
    "\n",
    "There are a number of (outdated) ranking formulas for HN publicly available. Take a look at least one of them and perform a regression using the formula to see if least squares regression can compute the coefficients correctly.\n",
    "\n",
    "Include plots showing your regression (for the functions of 1 or 2 variables and your predicted score for an article). What do your regressions tell you about making the front page?\n",
    "\n",
    "An example of my computed score vs the actual ranks can be seen here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a new column for whether something is on the front page\n",
    "submissions_df['onFrontPage'] = [0 if x > 30 else 1 for x in submissions_df['rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "colors = {0:'red', 1: 'blue'}\n",
    "rank_point_plt = plt.scatter(x=submissions_df['rank'], y=submissions_df['points'], c=submissions_df['onFrontPage'].map(colors))\n",
    "logistic = lambda x: 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "Score2 = []\n",
    "for p,t in zip(submissions_df['points'],submissions_df['ageInHours']):\n",
    "    Score2.append((p-1)/((t+2)**1.8))\n",
    "submissions_df['Score2'] = Score2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(y='Score2',x='rank',hue='onFrontPage',data=submissions_df)\n",
    "xs = np.linspace(submissions_df['Score2'][30], submissions_df['Score2'][30], 150)\n",
    "plt.plot(xs, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x='Score2', y='onFrontPage',data=submissions_df)\n",
    "log_model = sm.logit(formula=\"onFrontPage ~ Score2\", data=submissions_df).fit()\n",
    "logistic = lambda x: 1 / ( 1 + np.exp(-x) )\n",
    "\n",
    "x = np.linspace(submissions_df['Score2'].min(), submissions_df['Score2'].max(), 1000)\n",
    "par = dict(log_model.params)\n",
    "plt.plot(x, logistic(par['Intercept'] + par['Score2'] * x), color='Black')\n",
    "\n",
    "prob_top_30 = 0.50\n",
    "estimated_score_toPass = np.log(0.5) - par['Intercept'] / par['Score2']\n",
    "\n",
    "plt.plot( [estimated_score_toPass, estimated_score_toPass], [0, 1], 'r' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do your regressions tell you about making the front page?\n",
    "\n",
    "our data appears to be underfitted, indicating my equation for my Score2 doesn't contain all of the variables influencing what makes it on the front page, which makes sense\n",
    "HackerNews probably has a much more complex algorithm that takes many factors into account that are not part of my predictive model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
